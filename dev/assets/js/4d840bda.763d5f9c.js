"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[7356],{4802:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"Python/Gemini/Api/chat-conversation","title":"Conversational AI","description":"This document outlines how to create and manage conversational sessions using the google.genai library in Python. It covers initializing chat sessions, sending messages, overriding parameters for single requests, and importantly, how to permanently update parameters like temperature and maxoutputtokens while preserving system_instruction and existing history.","source":"@site/docs/Python/Gemini/Api/chat-conversation.md","sourceDirName":"Python/Gemini/Api","slug":"/Python/Gemini/Api/chat-conversation","permalink":"/dev/docs/Python/Gemini/Api/chat-conversation","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"python","permalink":"/dev/docs/tags/python"},{"inline":true,"label":"genai","permalink":"/dev/docs/tags/genai"},{"inline":true,"label":"conversation","permalink":"/dev/docs/tags/conversation"},{"inline":true,"label":"chat","permalink":"/dev/docs/tags/chat"},{"inline":true,"label":"dynamic configuration","permalink":"/dev/docs/tags/dynamic-configuration"},{"inline":true,"label":"parameter preservation","permalink":"/dev/docs/tags/parameter-preservation"}],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Conversational AI","sidebar_position":2,"tags":["python","genai","conversation","chat","dynamic configuration","parameter preservation"]},"sidebar":"tutorialSidebar","previous":{"title":"AI Chat Sessions","permalink":"/dev/docs/Python/Gemini/Api/chat-api"},"next":{"title":"Preserve Chat Configuration Updates","permalink":"/dev/docs/Python/Gemini/Api/preserve-chat-config"}}');var i=t(4848),s=t(8453);const a={title:"Conversational AI",sidebar_position:2,tags:["python","genai","conversation","chat","dynamic configuration","parameter preservation"]},o=void 0,c={},l=[{value:"Prerequisites",id:"prerequisites",level:3},{value:"1. Initialize the Client",id:"1-initialize-the-client",level:3},{value:"2. Create a Conversation with Initial Parameters",id:"2-create-a-conversation-with-initial-parameters",level:3},{value:"3. Sending Messages and Per-Call Parameter Overrides",id:"3-sending-messages-and-per-call-parameter-overrides",level:3},{value:"4. Updating Persistent Chat Parameters (Preserving State)",id:"4-updating-persistent-chat-parameters-preserving-state",level:3}];function p(e){const n={code:"code",em:"em",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["This document outlines how to create and manage conversational sessions using the ",(0,i.jsx)(n.code,{children:"google.genai"})," library in Python. It covers initializing chat sessions, sending messages, overriding parameters for single requests, and importantly, how to permanently update parameters like ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," while preserving ",(0,i.jsx)(n.code,{children:"system_instruction"})," and existing ",(0,i.jsx)(n.code,{children:"history"}),"."]}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.p,{children:["To run the examples, ensure you have the ",(0,i.jsx)(n.code,{children:"google-generative-ai"})," library installed. You will also need an API key for the Gemini API or appropriate Vertex AI credentials. For the Gemini API, you can set the ",(0,i.jsx)(n.code,{children:"GOOGLE_API_KEY"})," environment variable or pass it directly to the ",(0,i.jsx)(n.code,{children:"genai.Client"})," constructor."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"pip install google-generative-ai\n"})}),"\n",(0,i.jsx)(n.h3,{id:"1-initialize-the-client",children:"1. Initialize the Client"}),"\n",(0,i.jsxs)(n.p,{children:["First, initialize the ",(0,i.jsx)(n.code,{children:"Client"})," object. This is the entry point for interacting with the Google Generative AI services."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import google.genai as genai\r\nfrom google.genai import types\r\n\r\n# Configure your API key or Vertex AI credentials\r\n# Option 1: Set GOOGLE_API_KEY environment variable\r\n# export GOOGLE_API_KEY="YOUR_API_KEY"\r\n# client = genai.Client()\r\n\r\n# Option 2: Pass API key directly (for Gemini API)\r\nclient = genai.Client(api_key="YOUR_API_KEY")\r\n\r\n# Option 3: For Vertex AI (requires project and location)\r\n# client = genai.Client(vertexai=True, project="your-gcp-project-id", location="us-central1")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-create-a-conversation-with-initial-parameters",children:"2. Create a Conversation with Initial Parameters"}),"\n",(0,i.jsxs)(n.p,{children:["You can start a conversation (chat session) with an existing history and specify initial generation parameters such as ",(0,i.jsx)(n.code,{children:"temperature"}),", ",(0,i.jsx)(n.code,{children:"max_output_tokens"}),", ",(0,i.jsx)(n.code,{children:"top_p"}),", ",(0,i.jsx)(n.code,{children:"top_k"}),", and a ",(0,i.jsx)(n.code,{children:"system_instruction"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"client.chats.create()"})," method is used to initiate a ",(0,i.jsx)(n.code,{children:"Chat"})," object."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model"}),': Specifies the generative model to use (e.g., "gemini-pro").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"history"}),": An optional list of ",(0,i.jsx)(n.code,{children:"types.Content"})," objects representing previous turns in the conversation. Use ",(0,i.jsx)(n.code,{children:"types.UserContent()"})," and ",(0,i.jsx)(n.code,{children:"types.ModelContent()"})," for cleaner history definition."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"config"}),": An optional ",(0,i.jsx)(n.code,{children:"types.GenerateContentConfig"})," object to set initial generation parameters for the chat session. This ",(0,i.jsx)(n.code,{children:"config"})," will serve as the default for all subsequent messages in this session."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Define an optional initial conversation history\r\ninitial_history = [\r\n    types.UserContent("Hello!"),\r\n    types.ModelContent("Hi there! How can I assist you today?"),\r\n    types.UserContent("I\'m interested in learning about large language models."),\r\n]\r\n\r\n# Define initial generation configuration parameters\r\ninitial_generation_config = types.GenerateContentConfig(\r\n    temperature=0.7,          # Controls randomness: 0.0 (less random) to 1.0 (more random)\r\n    max_output_tokens=150,    # Maximum number of tokens to generate in the response\r\n    top_p=0.9,                # Tokens are sampled until their probabilities sum up to this value\r\n    top_k=40,                 # For each step, consider top_k tokens with highest probabilities\r\n    system_instruction=types.Content( # Use types.Content for system_instruction\r\n        parts=[types.Part(text="You are an expert in AI and machine learning. Provide detailed and informative answers.")]\r\n    )\r\n)\r\n\r\n# Create the chat session\r\nchat = client.chats.create(\r\n    model="gemini-pro", # Use an appropriate model, e.g., "gemini-pro"\r\n    history=initial_history,\r\n    config=initial_generation_config\r\n)\r\n\r\nprint("--- Initial Chat Session Configuration ---")\r\nprint(f"Model: {chat._model}")\r\n# Note: Accessing `_config` directly (e.g., `chat._config`) is for demonstration of internal state.\r\n# The `send_message` method internally uses this config as default.\r\nprint(f"Default Temperature: {chat._config.temperature}")\r\nprint(f"Default Max Output Tokens: {chat._config.max_output_tokens}")\r\nprint(f"Default System Instruction: {chat._config.system_instruction.parts[0].text if chat._config.system_instruction else \'None\'}")\r\n\r\nprint("\\n--- Initial Chat History (curated) ---")\r\nfor i, content in enumerate(chat.get_history(curated=True)):\r\n    role = content.role\r\n    # Check if content.parts exists and has a text attribute before accessing\r\n    text_content = content.parts[0].text if content.parts and hasattr(content.parts[0], \'text\') else "[Non-text content]"\r\n    print(f"Turn {i+1} ({role}): {text_content}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-sending-messages-and-per-call-parameter-overrides",children:"3. Sending Messages and Per-Call Parameter Overrides"}),"\n",(0,i.jsxs)(n.p,{children:["After a conversation is initiated, you can send new messages. The ",(0,i.jsx)(n.code,{children:"send_message()"})," method (or ",(0,i.jsx)(n.code,{children:"send_message_stream()"})," for streaming responses) allows you to override generation parameters ",(0,i.jsx)(n.strong,{children:"for that specific call only"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["You can modify parameters like ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," for individual ",(0,i.jsx)(n.code,{children:"send_message"})," calls by passing a new ",(0,i.jsx)(n.code,{children:"types.GenerateContentConfig"})," object to the ",(0,i.jsx)(n.code,{children:"config"})," argument. Any parameter set in this per-call ",(0,i.jsx)(n.code,{children:"config"})," will take precedence over the chat session's default for that particular message, but will ",(0,i.jsx)(n.strong,{children:"not"})," change the session's default for subsequent messages."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import time\r\n\r\n# Send a message, overriding temperature and max_output_tokens for this turn\r\nprint("\\n--- Sending Message 1 (overriding config for this call) ---")\r\nmessage_1_config = types.GenerateContentConfig(\r\n    temperature=0.2,       # Lower temperature for less randomness on this call\r\n    max_output_tokens=80   # Generate a shorter response for this call\r\n)\r\nresponse1 = chat.send_message("What are some applications of LLMs?", config=message_1_config)\r\nprint(f"User: What are some applications of LLMs?")\r\nprint(f"Model: {response1.text}")\r\nprint(f"Config for Message 1 (per-call override): Temperature={message_1_config.temperature}, Max Output Tokens={message_1_config.max_output_tokens}")\r\n\r\n# The chat\'s internal default config remains unchanged by the above call:\r\n# print(f"Chat\'s default temperature: {chat._config.temperature}")\r\n\r\n# Send another message, overriding with different parameters\r\nprint("\\n--- Sending Message 2 (overriding config again for this call) ---")\r\nmessage_2_config = types.GenerateContentConfig(\r\n    temperature=0.9,       # Higher temperature for more creative response on this call\r\n    max_output_tokens=120  # Allow for a longer response on this call\r\n)\r\nresponse2 = chat.send_message("Give me a creative example.", config=message_2_config)\r\nprint(f"User: Give me a creative example.")\r\nprint(f"Model: {response2.text}")\r\nprint(f"Config for Message 2 (per-call override): Temperature={message_2_config.temperature}, Max Output Tokens={message_2_config.max_output_tokens}")\r\n\r\n# Send a message without overriding config, which will use the initial_generation_config\r\nprint("\\n--- Sending Message 3 (using initial default config) ---")\r\nresponse3 = chat.send_message("Summarize our conversation so far.")\r\nprint(f"User: Summarize our conversation so far.")\r\nprint(f"Model: {response3.text}")\r\n# This call uses the temperature and max_output_tokens from `initial_generation_config`\r\nprint(f"Config for Message 3 (default from chat session): Temperature={initial_generation_config.temperature}, Max Output Tokens={initial_generation_config.max_output_tokens}")\r\n\r\n# Demonstrate streaming response\r\nprint("\\n--- Sending Message 4 (streaming, overriding config for this call) ---")\r\nmessage_4_config = types.GenerateContentConfig(\r\n    temperature=0.5,\r\n    max_output_tokens=60\r\n)\r\nprint(f"User: Tell me a very short story about a brave knight.")\r\nprint("Model (streaming): ", end="")\r\nfor chunk in chat.send_message_stream("Tell me a very short story about a brave knight.", config=message_4_config):\r\n    print(chunk.text, end="", flush=True)\r\n    time.sleep(0.05) # Simulate real-time printing\r\nprint("\\n")\r\nprint(f"Config for Message 4 (per-call override): Temperature={message_4_config.temperature}, Max Output Tokens={message_4_config.max_output_tokens}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"4-updating-persistent-chat-parameters-preserving-state",children:"4. Updating Persistent Chat Parameters (Preserving State)"}),"\n",(0,i.jsxs)(n.p,{children:["The core configuration of a ",(0,i.jsx)(n.code,{children:"Chat"})," object, including its ",(0,i.jsx)(n.code,{children:"system_instruction"})," and default generation parameters, is set during its creation and is not directly modifiable through public methods. To make changes to ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," that persist for all ",(0,i.jsx)(n.em,{children:"future"})," messages in a conversation (while retaining the ",(0,i.jsx)(n.code,{children:"system_instruction"})," and accumulated chat history), you must ",(0,i.jsxs)(n.strong,{children:["recreate the ",(0,i.jsx)(n.code,{children:"Chat"})," instance"]}),"."]}),"\n",(0,i.jsx)(n.p,{children:"This process involves:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Retrieving Current State:"})," Get the existing model name, the current ",(0,i.jsx)(n.code,{children:"GenerateContentConfig"})," (which includes ",(0,i.jsx)(n.code,{children:"system_instruction"}),"), and the full conversation history from the active ",(0,i.jsx)(n.code,{children:"Chat"})," instance."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gathering New Parameters:"})," Obtain the new ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," values."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Merging Configurations:"})," Combine the existing ",(0,i.jsx)(n.code,{children:"GenerateContentConfig"})," values with the new parameter values into a single, updated configuration."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Recreating the ",(0,i.jsx)(n.code,{children:"Chat"})," Instance:"]})," Create a new ",(0,i.jsx)(n.code,{children:"Chat"})," object using ",(0,i.jsx)(n.code,{children:"client.chats.create()"}),", passing the original model, the newly merged configuration, and the preserved chat history."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Replacing the Old Instance:"})," Update your program's reference to the ",(0,i.jsx)(n.code,{children:"Chat"})," object to point to the new instance."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This ensures a seamless transition to the new parameters without losing any conversational context or other predefined settings."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Function to get new parameters from user (can be integrated into your application\'s UI)\r\ndef get_user_param_overrides():\r\n    """Prompts the user for temperature and max_output_tokens, allowing for empty input to keep current."""\r\n    print("\\n--- Enter New Model Parameters (Leave empty to keep current) ---")\r\n    temp_input = input("New Temperature (float, e.g., 0.7): ")\r\n    max_tokens_input = input("New Max Output Tokens (int, e.g., 200): ")\r\n\r\n    new_temperature = None\r\n    if temp_input:\r\n        try:\r\n            new_temperature = float(temp_input)\r\n            if not (0.0 <= new_temperature <= 1.0):\r\n                print("Note: Temperature usually ranges from 0.0 to 1.0 for typical use cases.")\r\n        except ValueError:\r\n            print("Invalid input for temperature. Keeping current value.")\r\n\r\n    new_max_output_tokens = None\r\n    if max_tokens_input:\r\n        try:\r\n            new_max_output_tokens = int(max_tokens_input)\r\n            if new_max_output_tokens <= 0:\r\n                print("Note: Max output tokens must be a positive integer.")\r\n        except ValueError:\r\n            print("Invalid input for max_output_tokens. Keeping current value.")\r\n\r\n    return new_temperature, new_max_output_tokens\r\n\r\ndef recreate_chat_with_updates(current_chat: genai.chats.Chat) -> genai.chats.Chat:\r\n    """\r\n    Retrieves current chat state, merges with new parameters, and recreates the chat.\r\n    Returns the new Chat instance.\r\n    """\r\n    print("\\n--- Updating Chat Parameters Permanently ---")\r\n\r\n    # 1. Retrieve current model name, configuration, and history\r\n    current_model_name = current_chat._model\r\n    # Accessing _config directly is a common pattern for inspecting internal state\r\n    # when no public getter is provided for the full config object.\r\n    current_gen_config: types.GenerateContentConfig = current_chat._config\r\n    current_history = current_chat.get_history(curated=False) # Retrieve comprehensive history\r\n\r\n    # 2. Get new parameter values from user or source\r\n    new_temperature, new_max_output_tokens = get_user_param_overrides()\r\n\r\n    # 3. Merge configurations\r\n    # Start with a dictionary representation of the current config to carry over all its fields\r\n    updated_config_dict = current_gen_config.model_dump()\r\n\r\n    # Apply new parameters if they were provided (not None)\r\n    if new_temperature is not None:\r\n        updated_config_dict[\'temperature\'] = new_temperature\r\n    if new_max_output_tokens is not None:\r\n        updated_config_dict[\'max_output_tokens\'] = new_max_output_tokens\r\n\r\n    # Create a new GenerateContentConfig object from the merged dictionary\r\n    final_gen_config = types.GenerateContentConfig(**updated_config_dict)\r\n\r\n    print("\\nRecreating chat session with updated configuration:")\r\n    print(f"  Model: {current_model_name}")\r\n    print(f"  New Temperature: {final_gen_config.temperature}")\r\n    print(f"  New Max Output Tokens: {final_gen_config.max_output_tokens}")\r\n    if final_gen_config.system_instruction:\r\n        print(f"  System Instruction: \'{final_gen_config.system_instruction.parts[0].text}\'")\r\n    print(f"  History length preserved: {len(current_history)} turns")\r\n\r\n    # 4. Recreate the chat session with the combined configuration and history\r\n    updated_chat = client.chats.create(\r\n        model=current_model_name,\r\n        config=final_gen_config,\r\n        history=current_history # Pass the entire preserved conversation history\r\n    )\r\n    return updated_chat\r\n\r\n# --- Main interactive chat loop to demonstrate persistent updates ---\r\n# This part is for demonstration and would integrate with your application\'s flow.\r\n\r\n# Define an initial chat session (as in section 2)\r\ninitial_system_instruction = types.Content(parts=[types.Part.from_text("You are a concise assistant. Provide short answers.")])\r\ninitial_history = [\r\n    types.UserContent("What is Python?"),\r\n    types.ModelContent("Python is a popular programming language.")\r\n]\r\ninitial_gen_config = types.GenerateContentConfig(\r\n    temperature=0.4,\r\n    max_output_tokens=50,\r\n    system_instruction=initial_system_instruction\r\n)\r\n\r\nprint("--- Initializing Main Chat Session ---")\r\ncurrent_chat_session = client.chats.create(\r\n    model=\'gemini-pro\',\r\n    config=initial_gen_config,\r\n    history=initial_history\r\n)\r\nprint(f"Current chat config: Temperature={current_chat_session._config.temperature}, MaxTokens={current_chat_session._config.max_output_tokens}")\r\nprint(f"Current system instruction: {current_chat_session._config.system_instruction.parts[0].text}")\r\nprint(f"Current history length: {len(current_chat_session.get_history())} turns")\r\n\r\nprint("\\n--- Start Chatting ---")\r\nprint("Type \'update_params\' to change temperature and max_output_tokens persistently.")\r\nprint("Type \'quit\' or \'exit\' to end the chat.")\r\n\r\nwhile True:\r\n    user_input = input("\\nYou: ")\r\n    if user_input.lower() in ["quit", "exit"]:\r\n        print("Ending chat session. Goodbye!")\r\n        break\r\n\r\n    if user_input.lower() == "update_params":\r\n        # Call the function to recreate the chat with updated parameters.\r\n        # The returned new chat instance replaces the old one.\r\n        current_chat_session = recreate_chat_with_updates(current_chat_session)\r\n        print("\\nChat session parameters updated. Continue typing messages.")\r\n        # Continue to the next loop iteration without sending \'update_params\' to the model\r\n        continue\r\n\r\n    try:\r\n        response = current_chat_session.send_message(user_input)\r\n        print(f"Model: {response.text}")\r\n    except Exception as e:\r\n        print(f"An error occurred during message generation: {e}")\r\n        break\r\n\r\nprint("\\n--- Final Chat History (comprehensive) ---")\r\n# The history property of the chat object keeps track of all turns\r\n# including the original history and all messages sent and received.\r\nfor i, content in enumerate(current_chat_session.get_history()):\r\n    role = content.role\r\n    text_content = content.parts[0].text if content.parts and hasattr(content.parts[0], \'text\') else "[Non-text content]"\r\n    print(f"Turn {i+1} ({role}): {text_content}")\r\n\r\nprint("\\n--- Final Chat History (curated) ---")\r\n# The curated history contains only valid turns that contribute to the model\'s context\r\nfor i, content in enumerate(current_chat_session.get_history(curated=True)):\r\n    role = content.role\r\n    text_content = content.parts[0].text if content.parts and hasattr(content.parts[0], \'text\') else "[Non-text content]"\r\n    print(f"Turn {i+1} ({role}): {text_content}")\n'})})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(6540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);