"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[6683],{8259:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>i,default:()=>h,frontMatter:()=>a,metadata:()=>o,toc:()=>p});const o=JSON.parse('{"id":"Python/Openai/Api/open-api-code-examples","title":"OpenAI Chat Completion","description":"The most common use case for the OpenAI API in Python is generating chat completions. This involves sending a list of messages to a specified model and receiving a generated response. The API supports both synchronous (blocking) and asynchronous (non-blocking) calls, and responses can be received as a single object or streamed as chunks.","source":"@site/docs/Python/Openai/Api/open-api-code-examples.md","sourceDirName":"Python/Openai/Api","slug":"/Python/Openai/Api/open-api-code-examples","permalink":"/dev/docs/Python/Openai/Api/open-api-code-examples","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"python","permalink":"/dev/docs/tags/python"},{"inline":true,"label":"openai","permalink":"/dev/docs/tags/openai"},{"inline":true,"label":"chat","permalink":"/dev/docs/tags/chat"},{"inline":true,"label":"api","permalink":"/dev/docs/tags/api"}],"version":"current","sidebarPosition":2,"frontMatter":{"title":"OpenAI Chat Completion","sidebar_position":2,"tags":["python","openai","chat","api"]},"sidebar":"tutorialSidebar","previous":{"title":"OpenAI API Uses","permalink":"/dev/docs/Python/Openai/Api/openai-api-intro"},"next":{"title":"OpenAI API Examples","permalink":"/dev/docs/Python/Openai/Api/openai-use-case"}}');var r=t(4848),s=t(8453);const a={title:"OpenAI Chat Completion",sidebar_position:2,tags:["python","openai","chat","api"]},i=void 0,c={},p=[{value:"Synchronous Chat Completion (Non-Streaming)",id:"synchronous-chat-completion-non-streaming",level:3},{value:"Synchronous Chat Completion (Streaming)",id:"synchronous-chat-completion-streaming",level:3},{value:"Asynchronous Chat Completion (Non-Streaming)",id:"asynchronous-chat-completion-non-streaming",level:3},{value:"Asynchronous Chat Completion (Streaming)",id:"asynchronous-chat-completion-streaming",level:3}];function l(n){const e={code:"code",h3:"h3",p:"p",pre:"pre",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.p,{children:"The most common use case for the OpenAI API in Python is generating chat completions. This involves sending a list of messages to a specified model and receiving a generated response. The API supports both synchronous (blocking) and asynchronous (non-blocking) calls, and responses can be received as a single object or streamed as chunks."}),"\n",(0,r.jsx)(e.h3,{id:"synchronous-chat-completion-non-streaming",children:"Synchronous Chat Completion (Non-Streaming)"}),"\n",(0,r.jsx)(e.p,{children:"This example demonstrates how to send a single prompt to the OpenAI API and receive the complete response in one go."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\r\n\r\n# Initialize the OpenAI client.\r\n# The API key is automatically loaded from the OPENAI_API_KEY environment variable.\r\n# Alternatively, you can pass it directly: client = OpenAI(api_key="your_api_key_here")\r\nclient = OpenAI()\r\n\r\ntry:\r\n    # Create a chat completion\r\n    chat_completion = client.chat.completions.create(\r\n        messages=[\r\n            {\r\n                "role": "user",\r\n                "content": "What is the capital of France?",\r\n            }\r\n        ],\r\n        model="gpt-4o", # Specify the model to use\r\n    )\r\n\r\n    # Access the content of the generated message\r\n    response_content = chat_completion.choices[0].message.content\r\n    print(f"Non-streaming response:\\n{response_content}")\r\n\r\nexcept Exception as e:\r\n    print(f"An error occurred: {e}")\n'})}),"\n",(0,r.jsx)(e.h3,{id:"synchronous-chat-completion-streaming",children:"Synchronous Chat Completion (Streaming)"}),"\n",(0,r.jsx)(e.p,{children:"This example shows how to receive the model's response incrementally as it is generated, which is useful for applications that want to display the response in real-time."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from openai import OpenAI\r\n\r\nclient = OpenAI()\r\n\r\nprint("Streaming response:")\r\ntry:\r\n    # Create a chat completion with streaming enabled\r\n    stream = client.chat.completions.create(\r\n        messages=[\r\n            {\r\n                "role": "user",\r\n                "content": "Write a short, uplifting poem about the beauty of nature.",\r\n            }\r\n        ],\r\n        model="gpt-4o", # Specify the model to use\r\n        stream=True, # Enable streaming\r\n    )\r\n\r\n    # Iterate over the stream to print chunks as they arrive\r\n    for chunk in stream:\r\n        # Each chunk contains a \'delta\' object with partial content\r\n        # Use .delta.content or "" to handle cases where content might be None\r\n        print(chunk.choices[0].delta.content or "", end="")\r\n    print("\\n") # Add a newline at the end for clean output\r\n\r\nexcept Exception as e:\r\n    print(f"An error occurred: {e}")\n'})}),"\n",(0,r.jsx)(e.h3,{id:"asynchronous-chat-completion-non-streaming",children:"Asynchronous Chat Completion (Non-Streaming)"}),"\n",(0,r.jsxs)(e.p,{children:["For asynchronous applications, the ",(0,r.jsx)(e.code,{children:"AsyncOpenAI"})," client can be used with ",(0,r.jsx)(e.code,{children:"await"}),"."]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import asyncio\r\nfrom openai import AsyncOpenAI\r\n\r\nasync def async_non_streaming_completion():\r\n    # Initialize the asynchronous OpenAI client\r\n    client = AsyncOpenAI()\r\n\r\n    try:\r\n        # Create an asynchronous chat completion\r\n        chat_completion = await client.chat.completions.create(\r\n            messages=[\r\n                {\r\n                    "role": "user",\r\n                    "content": "Summarize the main idea of quantum physics in one sentence.",\r\n                }\r\n            ],\r\n            model="gpt-4o",\r\n        )\r\n\r\n        response_content = chat_completion.choices[0].message.content\r\n        print(f"Async non-streaming response:\\n{response_content}")\r\n\r\n    except Exception as e:\r\n        print(f"An error occurred: {e}")\r\n\r\n# Run the asynchronous function\r\n# asyncio.run(async_non_streaming_completion())\n'})}),"\n",(0,r.jsx)(e.h3,{id:"asynchronous-chat-completion-streaming",children:"Asynchronous Chat Completion (Streaming)"}),"\n",(0,r.jsx)(e.p,{children:"Streaming responses are also available asynchronously, allowing for efficient real-time updates in async applications."}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import asyncio\r\nfrom openai import AsyncOpenAI\r\n\r\nasync def async_streaming_completion():\r\n    client = AsyncOpenAI()\r\n\r\n    print("Async streaming response:")\r\n    try:\r\n        # Create an asynchronous streaming chat completion\r\n        stream = await client.chat.completions.create(\r\n            messages=[\r\n                {\r\n                    "role": "user",\r\n                    "content": "Tell me a brief story about a brave knight and a dragon.",\r\n                }\r\n            ],\r\n            model="gpt-4o",\r\n            stream=True,\r\n        )\r\n\r\n        async for chunk in stream:\r\n            print(chunk.choices[0].delta.content or "", end="")\r\n        print("\\n")\r\n\r\n    except Exception as e:\r\n        print(f"An error occurred: {e}")\r\n\r\n# Run the asynchronous function\r\n# asyncio.run(async_streaming_completion())\n'})})]})}function h(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(l,{...n})}):l(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>i});var o=t(6540);const r={},s=o.createContext(r);function a(n){const e=o.useContext(s);return o.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),o.createElement(s.Provider,{value:e},n.children)}}}]);