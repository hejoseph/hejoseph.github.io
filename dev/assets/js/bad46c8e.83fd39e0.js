"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[2752],{948:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"Python/Gemini/Api/preserve-chat-config","title":"Preserve Chat Configuration Updates","description":"When working with the Gemini API\'s chat functionality, it\'s common to initialize a chat session with specific parameters such as temperature, maxoutputtokens, systeminstruction, and an initial history. A challenge arises when you later want to dynamically update only the temperature and maxoutputtokens without losing the previously set systeminstruction or the accumulated conversation history.","source":"@site/docs/Python/Gemini/Api/preserve-chat-config.md","sourceDirName":"Python/Gemini/Api","slug":"/Python/Gemini/Api/preserve-chat-config","permalink":"/dev/docs/Python/Gemini/Api/preserve-chat-config","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"Gemini API","permalink":"/dev/docs/tags/gemini-api"},{"inline":true,"label":"Chat Session Management","permalink":"/dev/docs/tags/chat-session-management"},{"inline":true,"label":"Dynamic Configuration","permalink":"/dev/docs/tags/dynamic-configuration"},{"inline":true,"label":"System Instruction","permalink":"/dev/docs/tags/system-instruction"},{"inline":true,"label":"Chat History","permalink":"/dev/docs/tags/chat-history"},{"inline":true,"label":"Model Parameters","permalink":"/dev/docs/tags/model-parameters"}],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Preserve Chat Configuration Updates","sidebar_position":3,"tags":["Gemini API","Chat Session Management","Dynamic Configuration","System Instruction","Chat History","Model Parameters"]},"sidebar":"tutorialSidebar","previous":{"title":"Conversational AI","permalink":"/dev/docs/Python/Gemini/Api/chat-conversation"},"next":{"title":"OpenAI API Uses","permalink":"/dev/docs/Python/Openai/Api/openai-api-intro"}}');var i=t(4848),s=t(8453);const o={title:"Preserve Chat Configuration Updates",sidebar_position:3,tags:["Gemini API","Chat Session Management","Dynamic Configuration","System Instruction","Chat History","Model Parameters"]},a=void 0,c={},l=[{value:"Key Components",id:"key-components",level:3},{value:"Step-by-Step Guide for Preserving Configuration",id:"step-by-step-guide-for-preserving-configuration",level:3},{value:"Example Code",id:"example-code",level:3}];function d(e){const n={code:"code",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["When working with the Gemini API's chat functionality, it's common to initialize a chat session with specific parameters such as ",(0,i.jsx)(n.code,{children:"temperature"}),", ",(0,i.jsx)(n.code,{children:"max_output_tokens"}),", ",(0,i.jsx)(n.code,{children:"system_instruction"}),", and an initial ",(0,i.jsx)(n.code,{children:"history"}),". A challenge arises when you later want to dynamically update only the ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," without losing the previously set ",(0,i.jsx)(n.code,{children:"system_instruction"})," or the accumulated conversation ",(0,i.jsx)(n.code,{children:"history"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The core principle to understand from the provided API structure is that a ",(0,i.jsx)(n.code,{children:"Chat"})," object's configuration (",(0,i.jsx)(n.code,{children:"_config"}),") and history (",(0,i.jsx)(n.code,{children:"_comprehensive_history"}),") are set during its initialization (",(0,i.jsx)(n.code,{children:"__init__"}),") and are not directly modifiable through public methods afterwards. Therefore, to achieve dynamic updates while preserving all existing settings, you must:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Retrieve"})," the current state of the existing ",(0,i.jsx)(n.code,{children:"Chat"})," session (its configuration and history)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Merge"})," the new ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," values with the retrieved configuration."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recreate"})," a new ",(0,i.jsx)(n.code,{children:"Chat"})," instance using the combined, updated configuration and the preserved history."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"google.genai.Client"}),": The main client for interacting with the Gemini API."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"google.genai.chats.Chat"}),": Represents an ongoing chat session."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"google.genai.types.GenerateContentConfig"}),": A Pydantic model (or ",(0,i.jsx)(n.code,{children:"GenerateContentConfigDict"})," for dictionary input) that encapsulates various model generation parameters, including ",(0,i.jsx)(n.code,{children:"temperature"}),", ",(0,i.jsx)(n.code,{children:"max_output_tokens"}),", and ",(0,i.jsx)(n.code,{children:"system_instruction"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"google.genai.types.Content"}),": Represents a piece of content in the conversation, used for both ",(0,i.jsx)(n.code,{children:"system_instruction"})," and ",(0,i.jsx)(n.code,{children:"history"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-by-step-guide-for-preserving-configuration",children:"Step-by-Step Guide for Preserving Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["Assume you have an existing ",(0,i.jsx)(n.code,{children:"chat"})," object that was created with an initial ",(0,i.jsx)(n.code,{children:"system_instruction"})," and has accumulated ",(0,i.jsx)(n.code,{children:"history"}),"."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Access Current Configuration and History:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The current ",(0,i.jsx)(n.code,{children:"GenerateContentConfig"})," is stored in the ",(0,i.jsx)(n.code,{children:"_config"})," attribute of the ",(0,i.jsx)(n.code,{children:"Chat"})," object."]}),"\n",(0,i.jsxs)(n.li,{children:["The current chat history is accessible via the ",(0,i.jsx)(n.code,{children:"get_history()"})," method of the ",(0,i.jsx)(n.code,{children:"Chat"})," object. It's recommended to retrieve the ",(0,i.jsx)(n.code,{children:"comprehensive"})," history (default ",(0,i.jsx)(n.code,{children:"curated=False"}),") to ensure all turns, including those with invalid outputs, are preserved for context."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# From an existing chat_session object\r\ncurrent_config = chat_session._config\r\ncurrent_history = chat_session.get_history(curated=False)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Prepare New Parameter Values:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Obtain the desired new ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," from the user or another source."]}),"\n",(0,i.jsx)(n.li,{children:"Implement logic to handle cases where the user might not want to change a specific parameter (e.g., allow empty input to keep the current value)."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Merge Configurations:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Convert the ",(0,i.jsx)(n.code,{children:"current_config"})," (which is a Pydantic model) into a mutable dictionary using ",(0,i.jsx)(n.code,{children:".model_dump()"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Update this dictionary with the new ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," values. The existing ",(0,i.jsx)(n.code,{children:"system_instruction"})," (and any other ",(0,i.jsx)(n.code,{children:"GenerateContentConfig"})," fields) will be automatically carried over from ",(0,i.jsx)(n.code,{children:"current_config"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Reconstruct a new ",(0,i.jsx)(n.code,{children:"types.GenerateContentConfig"})," object from this merged dictionary."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# new_temperature and new_max_output_tokens are obtained from user input\r\n# (e.g., None if the user didn't specify a change)\r\n\r\n# Convert current config to dictionary for merging\r\nnew_config_dict = current_config.model_dump() if current_config else {}\r\n\r\nif new_temperature is not None:\r\n    new_config_dict['temperature'] = new_temperature\r\nif new_max_output_tokens is not None:\r\n    new_config_dict['max_output_tokens'] = new_max_output_tokens\r\n\r\n# Create the final GenerateContentConfig object\r\nfinal_config = types.GenerateContentConfig(**new_config_dict)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Recreate the Chat Session:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Call ",(0,i.jsx)(n.code,{children:"client.chats.create()"})," again."]}),"\n",(0,i.jsxs)(n.li,{children:["Pass the original ",(0,i.jsx)(n.code,{children:"model"})," name (",(0,i.jsx)(n.code,{children:"chat_session._model"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:["Pass the ",(0,i.jsx)(n.code,{children:"final_config"})," that now contains all preserved and updated parameters."]}),"\n",(0,i.jsxs)(n.li,{children:["Pass the ",(0,i.jsx)(n.code,{children:"current_history"})," to ensure the conversational context is maintained."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"updated_chat_session = client.chats.create(\r\n    model=chat_session._model,\r\n    config=final_config,\r\n    history=current_history\r\n)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Replace the Old Chat Instance:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Crucially, you must replace your reference to the old ",(0,i.jsx)(n.code,{children:"chat_session"})," object with the ",(0,i.jsx)(n.code,{children:"updated_chat_session"})," object. All subsequent interactions will then use the new ",(0,i.jsx)(n.code,{children:"Chat"})," instance with the desired persistent parameters."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-code",children:"Example Code"}),"\n",(0,i.jsxs)(n.p,{children:["The following example demonstrates an interactive chat where users can type ",(0,i.jsx)(n.code,{children:"update_params"})," to change the ",(0,i.jsx)(n.code,{children:"temperature"})," and ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," while keeping the ",(0,i.jsx)(n.code,{children:"system_instruction"})," and ",(0,i.jsx)(n.code,{children:"history"})," intact."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import google.genai as genai\r\nfrom google.genai import types\r\nimport os\r\n\r\n# Ensure your API key is set as an environment variable or passed directly\r\n# os.environ["GOOGLE_API_KEY"] = "YOUR_API_KEY"\r\n\r\n# --- Client Initialization ---\r\ntry:\r\n    client = genai.Client()\r\nexcept Exception:\r\n    print("Google GenAI client could not be initialized.")\r\n    print("Please ensure GOOGLE_API_KEY environment variable is set or pass it directly.")\r\n    print("Using a mock client for demonstration purposes only if a real client isn\'t configured.")\r\n\r\n    # --- Mock Client for local testing without API key ---\r\n    class MockResponse:\r\n        def __init__(self, text):\r\n            self.text = text\r\n        def __str__(self):\r\n            return self.text\r\n\r\n    class MockChat:\r\n        def __init__(self, model: str, config: types.GenerateContentConfig, history: list[types.Content]):\r\n            self._model = model\r\n            self._config = config\r\n            self._comprehensive_history = history if history is not None else []\r\n            print(f"MockChat created: Model=\'{model}\', Temp={config.temperature}, MaxTokens={config.max_output_tokens}, History Length={len(self._comprehensive_history)}")\r\n            if config.system_instruction:\r\n                print(f"  System Instruction: \'{config.system_instruction.parts[0].text}\'")\r\n\r\n        def send_message(self, message: str):\r\n            # Append user message to mock history\r\n            self._comprehensive_history.append(types.Content(role=\'user\', parts=[types.Part.from_text(message)]))\r\n\r\n            # Simulate model response\r\n            response_text = (\r\n                f"Mock response (T={self._config.temperature}, MaxTok={self._config.max_output_tokens})"\r\n                f" to: \'{message}\'. Current system instruction: \'{self._config.system_instruction.parts[0].text}\'"\r\n            )\r\n            model_content = types.Content(role=\'model\', parts=[types.Part.from_text(response_text)])\r\n            self._comprehensive_history.append(model_content)\r\n\r\n            # Return a mock GenerateContentResponse\r\n            mock_gen_response = types.GenerateContentResponse(\r\n                candidates=[types.Candidate(content=model_content)]\r\n            )\r\n            return mock_gen_response\r\n\r\n        def get_history(self, curated: bool = False) -> list[types.Content]:\r\n            # For mock, always return comprehensive history\r\n            return self._comprehensive_history\r\n\r\n    class MockChats:\r\n        def create(self, model: str, config: types.GenerateContentConfig, history: list[types.Content] = None):\r\n            return MockChat(model=model, config=config, history=history)\r\n\r\n    class MockClient:\r\n        def __init__(self):\r\n            self.chats = MockChats()\r\n    client = MockClient()\r\n# --- End Mock Client ---\r\n\r\ndef get_user_param_overrides():\r\n    """Prompts the user for temperature and max_output_tokens, allowing for empty input."""\r\n    print("\\n--- Enter New Model Parameters (Leave empty to keep current) ---")\r\n    temp_input = input("New Temperature (float, e.g., 0.7): ")\r\n    max_tokens_input = input("New Max Output Tokens (int, e.g., 200): ")\r\n\r\n    new_temperature = None\r\n    if temp_input:\r\n        try:\r\n            new_temperature = float(temp_input)\r\n            if not (0.0 <= new_temperature <= 1.0):\r\n                print("Note: Temperature usually ranges from 0.0 to 1.0 for typical use cases.")\r\n        except ValueError:\r\n            print("Invalid input for temperature. Keeping current value.")\r\n            new_temperature = None # Reset to None if invalid\r\n\r\n    new_max_output_tokens = None\r\n    if max_tokens_input:\r\n        try:\r\n            new_max_output_tokens = int(max_tokens_input)\r\n            if new_max_output_tokens <= 0:\r\n                print("Note: Max output tokens must be a positive integer.")\r\n                new_max_output_tokens = None # Reset to None if invalid\r\n        except ValueError:\r\n            print("Invalid input for max_output_tokens. Keeping current value.")\r\n            new_max_output_tokens = None # Reset to None if invalid\r\n\r\n    return new_temperature, new_max_output_tokens\r\n\r\ndef recreate_chat_with_updates(current_chat: genai.chats.Chat) -> genai.chats.Chat:\r\n    """\r\n    Retrieves current chat state, merges with new parameters, and recreates the chat.\r\n    """\r\n    print("\\nAttempting to update chat parameters...")\r\n\r\n    # 1. Retrieve current configuration and history\r\n    # Accessing _config directly is necessary as there\'s no public getter\r\n    current_model_name = current_chat._model\r\n    current_gen_config: types.GenerateContentConfig = current_chat._config\r\n    current_history = current_chat.get_history(curated=False) # Get comprehensive history\r\n\r\n    # 2. Get new parameter values from user\r\n    new_temperature, new_max_output_tokens = get_user_param_overrides()\r\n\r\n    # 3. Merge configurations\r\n    # Start with a dictionary representation of the current config\r\n    updated_config_dict = current_gen_config.model_dump()\r\n\r\n    # Apply new parameters if provided by the user\r\n    if new_temperature is not None:\r\n        updated_config_dict[\'temperature\'] = new_temperature\r\n    if new_max_output_tokens is not None:\r\n        updated_config_dict[\'max_output_tokens\'] = new_max_output_tokens\r\n\r\n    # Create a new GenerateContentConfig object from the merged dictionary\r\n    final_gen_config = types.GenerateContentConfig(**updated_config_dict)\r\n\r\n    print("\\nRecreating chat session with:")\r\n    print(f"  Model: {current_model_name}")\r\n    print(f"  Temperature: {final_gen_config.temperature}")\r\n    print(f"  Max Output Tokens: {final_gen_config.max_output_tokens}")\r\n    if final_gen_config.system_instruction:\r\n        # Accessing parts[0].text is a simplification for typical text system instructions\r\n        print(f"  System Instruction: \'{final_gen_config.system_instruction.parts[0].text}\'")\r\n    print(f"  History length: {len(current_history)} turns")\r\n\r\n    # 4. Recreate the chat session\r\n    updated_chat = client.chats.create(\r\n        model=current_model_name,\r\n        config=final_gen_config,\r\n        history=current_history # Pass the preserved history\r\n    )\r\n    return updated_chat\r\n\r\ndef main_interactive_chat():\r\n    """Initializes and runs an interactive chat session."""\r\n\r\n    # --- Initial Chat Setup ---\r\n    # Define an initial system instruction and history\r\n    initial_system_instruction = types.Content(parts=[types.Part.from_text("You are a knowledgeable and friendly AI assistant. Keep responses concise unless asked for details.")])\r\n    initial_history = [\r\n        types.Content(role=\'user\', parts=[types.Part.from_text("Hi, what\'s your purpose?")]),\r\n        types.Content(role=\'model\', parts=[types.Part.from_text("I\'m here to assist you with information and tasks, designed by Google.")])\r\n    ]\r\n\r\n    # Define initial generation config\r\n    initial_gen_config = types.GenerateContentConfig(\r\n        temperature=0.7,\r\n        max_output_tokens=150,\r\n        system_instruction=initial_system_instruction\r\n    )\r\n\r\n    print("--- Initializing Chat Session ---")\r\n    current_chat_session = client.chats.create(\r\n        model=\'gemini-pro\', # Or your preferred model\r\n        config=initial_gen_config,\r\n        history=initial_history\r\n    )\r\n    print(f"Initial chat config: Temperature={current_chat_session._config.temperature}, MaxTokens={current_chat_session._config.max_output_tokens}")\r\n    print(f"Initial system instruction: {current_chat_session._config.system_instruction.parts[0].text}")\r\n    print(f"Initial history length: {len(current_chat_session.get_history())} turns")\r\n\r\n\r\n    print("\\n--- Start Chatting ---")\r\n    print("Type \'update_params\' to change temperature and max_output_tokens.")\r\n    print("Type \'quit\' or \'exit\' to end the chat.")\r\n\r\n    while True:\r\n        user_input = input("\\nYou: ")\r\n        if user_input.lower() in ["quit", "exit"]:\r\n            print("Ending chat session. Goodbye!")\r\n            break\r\n\r\n        if user_input.lower() == "update_params":\r\n            # Call the function to recreate the chat with updated parameters\r\n            current_chat_session = recreate_chat_with_updates(current_chat_session)\r\n            print("\\nChat session parameters updated. Continue typing messages.")\r\n            continue # Skip sending the "update_params" message to the model\r\n\r\n        try:\r\n            response = current_chat_session.send_message(user_input)\r\n            print(f"Model: {response.text}")\r\n        except Exception as e:\r\n            print(f"An error occurred during generation: {e}")\r\n            break\r\n\r\nif __name__ == "__main__":\r\n    main_interactive_chat()\n'})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var r=t(6540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);