"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[8226],{6017:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"tech-related/Chatbot/optimize-chat-memory-limit-token-reached","title":"Token Limit Management","description":"When working with large language model (LLM) APIs such as OpenAI\u2019s gpt-4, each API call is subject to a maximum token limit (e.g., 8192 or 128k tokens). As conversations grow longer, especially when maintaining history for context, you may quickly approach or exceed these limits.","source":"@site/docs/tech-related/Chatbot/optimize-chat-memory-limit-token-reached.md","sourceDirName":"tech-related/Chatbot","slug":"/tech-related/Chatbot/optimize-chat-memory-limit-token-reached","permalink":"/dev/docs/tech-related/Chatbot/optimize-chat-memory-limit-token-reached","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"LLM API","permalink":"/dev/docs/tags/llm-api"},{"inline":true,"label":"token limits","permalink":"/dev/docs/tags/token-limits"},{"inline":true,"label":"conversation memory","permalink":"/dev/docs/tags/conversation-memory"}],"version":"current","sidebarPosition":7,"frontMatter":{"title":"Token Limit Management","sidebar_position":7,"tags":["LLM API","token limits","conversation memory"]},"sidebar":"tutorialSidebar","previous":{"title":"Max Token","permalink":"/dev/docs/tech-related/Chatbot/understand-models-max-token"},"next":{"title":"Clean Code Principles","permalink":"/dev/docs/tech-related/Clean Code/clean-code-basics"}}');var i=s(4848),r=s(8453);const a={title:"Token Limit Management",sidebar_position:7,tags:["LLM API","token limits","conversation memory"]},l="Managing Token Limits in AI Conversations",o={},d=[{value:"Why Token Limits Matter",id:"why-token-limits-matter",level:2},{value:"Strategies for Handling Token Limits",id:"strategies-for-handling-token-limits",level:2},{value:"1. Truncate Old Messages",id:"1-truncate-old-messages",level:3},{value:"2. Summarize Old Messages",id:"2-summarize-old-messages",level:3},{value:"3. Hybrid Strategy (Recommended)",id:"3-hybrid-strategy-recommended",level:3},{value:"4. Retrieval-Augmented Memory (Advanced)",id:"4-retrieval-augmented-memory-advanced",level:3},{value:"Choosing the Right Approach",id:"choosing-the-right-approach",level:2},{value:"Example: Summarize Then Continue",id:"example-summarize-then-continue",level:2},{value:"Final Tips",id:"final-tips",level:2}];function c(e){const n={br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"managing-token-limits-in-ai-conversations",children:"Managing Token Limits in AI Conversations"})}),"\n",(0,i.jsxs)(n.p,{children:["When working with large language model (LLM) APIs such as OpenAI\u2019s ",(0,i.jsx)(n.code,{children:"gpt-4"}),", each API call is subject to a ",(0,i.jsx)(n.strong,{children:"maximum token limit"})," (e.g., 8192 or 128k tokens). As conversations grow longer, especially when maintaining history for context, you may quickly approach or exceed these limits."]}),"\n",(0,i.jsxs)(n.p,{children:["This guide explains ",(0,i.jsx)(n.strong,{children:"why token limits are important"})," and provides ",(0,i.jsx)(n.strong,{children:"practical strategies"})," for maintaining context while staying within token constraints."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"why-token-limits-matter",children:"Why Token Limits Matter"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"All messages in the conversation history"})," (user and assistant) consume tokens."]}),"\n",(0,i.jsx)(n.li,{children:"If the total token count exceeds the model\u2019s limit, the API call will fail."}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"max_tokens"})," parameter determines how much space is reserved for the model\u2019s response."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example:"}),(0,i.jsx)(n.br,{}),"\n","If you use GPT-4 (8192 tokens max) and your prompt already uses 7800 tokens, you will only receive a very short response, or the call may fail."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"strategies-for-handling-token-limits",children:"Strategies for Handling Token Limits"}),"\n",(0,i.jsx)(n.h3,{id:"1-truncate-old-messages",children:"1. Truncate Old Messages"}),"\n",(0,i.jsx)(n.p,{children:"Remove the earliest messages from the conversation history (except the system message) to make room for new content."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"while token_count(messages + [new_prompt]) > MAX_TOKENS - reserved_for_response:\r\n    # Remove the oldest user/assistant message\r\n    messages.pop(1)\n"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pros:"})," Simple to implement"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cons:"})," Early context is lost"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"2-summarize-old-messages",children:"2. Summarize Old Messages"}),"\n",(0,i.jsx)(n.p,{children:"Before removing messages, ask the model to summarize them. Replace the removed chunk with a summary message."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'[\r\n  { "role": "system", "content": "You are David Goggins..." },\r\n  { "role": "assistant", "content": "Summary: user talked about weakness, pain, and you told him to use pain as fuel." },\r\n  { "role": "user", "content": "I\'m struggling again. What should I do?" }\r\n]\n'})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pros:"})," Retains intent and memory"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cons:"})," Requires summarization logic or an extra API call"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"3-hybrid-strategy-recommended",children:"3. Hybrid Strategy (Recommended)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Keep the most recent 5\u201310 messages in full"}),"\n",(0,i.jsx)(n.li,{children:"Summarize older parts into a single message"}),"\n",(0,i.jsx)(n.li,{children:"Use both raw and summarized context"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Best for:"})," Long-form chats, character roleplay, coaching bots"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"4-retrieval-augmented-memory-advanced",children:"4. Retrieval-Augmented Memory (Advanced)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Store all past conversations externally (e.g., in a vector database)"}),"\n",(0,i.jsx)(n.li,{children:"Retrieve the most relevant parts for each new request"}),"\n",(0,i.jsx)(n.li,{children:"Inject only those snippets into the prompt"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tools:"})," FAISS, Chroma, LangChain, LlamaIndex"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pros:"})," Best for long-term scaling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cons:"})," Requires backend and indexing logic"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"choosing-the-right-approach",children:"Choosing the Right Approach"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Strategy"}),(0,i.jsx)(n.th,{children:"Context Retention"}),(0,i.jsx)(n.th,{children:"Simplicity"}),(0,i.jsx)(n.th,{children:"Scalability"}),(0,i.jsx)(n.th,{children:"Use Case"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Delete old messages"}),(0,i.jsx)(n.td,{children:"Low"}),(0,i.jsx)(n.td,{children:"Simple"}),(0,i.jsx)(n.td,{children:"Limited"}),(0,i.jsx)(n.td,{children:"Short tasks, transactional queries"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Summarize & replace"}),(0,i.jsx)(n.td,{children:"High"}),(0,i.jsx)(n.td,{children:"Medium"}),(0,i.jsx)(n.td,{children:"Good"}),(0,i.jsx)(n.td,{children:"Coaching, journaling, persona-based chats"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Hybrid (raw + summary)"}),(0,i.jsx)(n.td,{children:"Excellent"}),(0,i.jsx)(n.td,{children:"Moderate"}),(0,i.jsx)(n.td,{children:"Best"}),(0,i.jsx)(n.td,{children:"Long-term assistants, memory-enhanced bots"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"Retrieval-Augmented"}),(0,i.jsx)(n.td,{children:"Excellent"}),(0,i.jsx)(n.td,{children:"Complex"}),(0,i.jsx)(n.td,{children:"Best"}),(0,i.jsx)(n.td,{children:"Scalable apps with memory or knowledge lookup"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"example-summarize-then-continue",children:"Example: Summarize Then Continue"}),"\n",(0,i.jsx)(n.p,{children:"When conversation history grows too long:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Summarize past messages"}),"\n",(0,i.jsx)(n.li,{children:"Insert the summary as an assistant message"}),"\n",(0,i.jsx)(n.li,{children:"Continue with new prompts"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'messages = [\r\n  { "role": "system", "content": "You are David Goggins..." },\r\n  { "role": "assistant", "content": "Summary of previous 50 messages..." },\r\n  { "role": "user", "content": "I\'m slipping again. Help me get back." }\r\n]\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"final-tips",children:"Final Tips"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Always reserve ",(0,i.jsx)(n.code,{children:"max_tokens"})," for the assistant's reply (e.g., 300\u20131000 tokens)"]}),"\n",(0,i.jsxs)(n.li,{children:["Use libraries like ",(0,i.jsx)(n.code,{children:"tiktoken"})," to count tokens before sending"]}),"\n",(0,i.jsx)(n.li,{children:"Refresh summaries every 20\u201330 messages to maintain context quality"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>l});var t=s(6540);const i={},r=t.createContext(i);function a(e){const n=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);