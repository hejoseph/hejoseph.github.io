"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[6890],{8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>o});var s=r(6540);const t={},i=s.createContext(t);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(i.Provider,{value:n},e.children)}},9052:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"tech-related/Langchain/langchain-how-does-it-work","title":"LangChain Workflow","description":"LangChain operates as a framework to build applications powered by language models (LLMs). Here\u2019s a technical breakdown of how LangChain works:","source":"@site/docs/tech-related/Langchain/langchain-how-does-it-work.md","sourceDirName":"tech-related/Langchain","slug":"/tech-related/Langchain/langchain-how-does-it-work","permalink":"/dev/docs/tech-related/Langchain/langchain-how-does-it-work","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"langchain","permalink":"/dev/docs/tags/langchain"},{"inline":true,"label":"large language models","permalink":"/dev/docs/tags/large-language-models"},{"inline":true,"label":"orchestration","permalink":"/dev/docs/tags/orchestration"}],"version":"current","sidebarPosition":4,"frontMatter":{"title":"LangChain Workflow","sidebar_position":4,"tags":["langchain","large language models","orchestration"]},"sidebar":"tutorialSidebar","previous":{"title":"LangChain Evolution","permalink":"/dev/docs/tech-related/Langchain/langchain-before-after"},"next":{"title":"API vs LangChain","permalink":"/dev/docs/tech-related/Langchain/langchain-benefits"}}');var t=r(4848),i=r(8453);const a={title:"LangChain Workflow",sidebar_position:4,tags:["langchain","large language models","orchestration"]},o=void 0,l={},c=[{value:"Core Workflow",id:"core-workflow",level:2},{value:"Key Architectural Components",id:"key-architectural-components",level:2},{value:"Example (Simplified)",id:"example-simplified",level:2},{value:"Summary",id:"summary",level:2}];function h(e){const n={br:"br",code:"code",em:"em",h2:"h2",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:"LangChain operates as a framework to build applications powered by language models (LLMs). Here\u2019s a technical breakdown of how LangChain works:"}),"\n",(0,t.jsx)(n.h2,{id:"core-workflow",children:"Core Workflow"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Input Processing:"}),(0,t.jsx)(n.br,{}),"\n","The user input (question, prompt, document, etc.) is received by LangChain's pipeline."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Prompt Construction:"}),(0,t.jsx)(n.br,{}),"\n","LangChain utilizes prompt templates, enabling dynamic insertion of variables and context based on the input type and task requirements."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Chain Assembly:"}),(0,t.jsx)(n.br,{}),"\n","A ",(0,t.jsx)(n.em,{children:"Chain"})," is configured, which defines a sequence of actions\u2014e.g., querying LLMs, using tool APIs, or retrieving documents. Chains may be:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simple (LLMChain):"})," Pass prompt to the model, get result."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Composite (SequentialChain, RouterChain):"})," Combine multiple steps, route depending on input."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tool/Component Integration:"}),(0,t.jsx)(n.br,{}),"\n","LangChain integrates external tools via ",(0,t.jsx)(n.em,{children:"Tool"})," or ",(0,t.jsx)(n.em,{children:"Agent"})," interfaces, such as:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Search engines"}),"\n",(0,t.jsx)(n.li,{children:"APIs (math, weather, databases)"}),"\n",(0,t.jsx)(n.li,{children:"Document loaders (PDFs, web pages)\r\nThese let the model augment its answers with external knowledge or computations."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Retriever/Memory Usage:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Retriever:"})," Used for fetching relevant documents from a dataset (vector stores, databases) using context (e.g., via similarity search)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory:"})," Stores previous dialogue or interactions for context continuity."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Model Invocation:"}),(0,t.jsx)(n.br,{}),"\n","The preprocessed prompt is sent to the configured LLM (OpenAI, Azure, Cohere, etc.) via their APIs, or locally, in the case of open source models."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Post-processing:"}),(0,t.jsx)(n.br,{}),"\n","Received outputs are parsed, reformatted, or piped through further chains or tools as defined in the workflow (e.g., extracting answers, summarizing, chaining tasks)."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Output Delivery:"}),(0,t.jsx)(n.br,{}),"\n","The final output\u2014answer, enriched text, or an action\u2014is delivered to the user or downstream component."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"key-architectural-components",children:"Key Architectural Components"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PromptTemplate:"})," Dynamically crafts prompts."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"LLM/ChatModel:"})," Encapsulates the actual language model."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chain:"})," Orchestrates sequential/compositional calls."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tool/Agent:"})," Plugins to interact with external resources. Agents decide which tools to invoke based on LLM-generated reasoning."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Memory:"})," Manages short-term (recent chat) and long-term context/history."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"example-simplified",children:"Example (Simplified)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from langchain.llms import OpenAI\r\nfrom langchain.chains import LLMChain\r\nfrom langchain.prompts import PromptTemplate\r\n\r\ntemplate = "What are the three largest countries by area?"\r\nprompt = PromptTemplate(template=template)\r\nllm = OpenAI()\r\nchain = LLMChain(llm=llm, prompt=prompt)\r\n\r\nresult = chain.run({})\n'})}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"LangChain orchestrates LLM applications by processing input, smartly assembling prompts and chains, integrating tools/memory, and managing complex workflows\u2014enabling robust, composable, context-aware language model operations."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);