"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[1661],{7353:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"Python/Gemini/Api/chat-conversation","title":"chat-conversation","description":"Ce document explique comment cr\xe9er et g\xe9rer des sessions conversationnelles \xe0 l\'aide de la biblioth\xe8que google.genai en Python. Il couvre l\'initialisation des sessions de discussion, l\'envoi de messages, la surcharge de param\xe8tres pour des requ\xeates individuelles, et, ce qui est important, comment mettre \xe0 jour de mani\xe8re permanente des param\xe8tres tels que temperature et maxoutputtokens tout en pr\xe9servant l\'system_instruction et l\'history existant.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/Python/Gemini/Api/chat-conversation.md","sourceDirName":"Python/Gemini/Api","slug":"/Python/Gemini/Api/chat-conversation","permalink":"/dev/fr/docs/Python/Gemini/Api/chat-conversation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"titre":"IA conversationnelle","position_dans_la_barre_lat\xe9rale":2,"balises":["python","genai","conversation","chat","configuration dynamique","pr\xe9servation des param\xe8tres"]},"sidebar":"tutorialSidebar","previous":{"title":"chat-api","permalink":"/dev/fr/docs/Python/Gemini/Api/chat-api"}}');var s=t(4848),i=t(8453);const o={titre:"IA conversationnelle","position_dans_la_barre_lat\xe9rale":2,balises:["python","genai","conversation","chat","configuration dynamique","pr\xe9servation des param\xe8tres"]},a=void 0,u={},l=[{value:"Pr\xe9requis",id:"pr\xe9requis",level:3},{value:"1. Initialiser le Client",id:"1-initialiser-le-client",level:3},{value:"2. Cr\xe9er une conversation avec des param\xe8tres initiaux",id:"2-cr\xe9er-une-conversation-avec-des-param\xe8tres-initiaux",level:3},{value:"3. Envoi de messages et surcharges de param\xe8tres par appel",id:"3-envoi-de-messages-et-surcharges-de-param\xe8tres-par-appel",level:3},{value:"4. Mise \xe0 jour des param\xe8tres de discussion persistants (pr\xe9servation de l&#39;\xe9tat)",id:"4-mise-\xe0-jour-des-param\xe8tres-de-discussion-persistants-pr\xe9servation-de-l\xe9tat",level:3}];function c(e){const n={code:"code",em:"em",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["Ce document explique comment cr\xe9er et g\xe9rer des sessions conversationnelles \xe0 l'aide de la biblioth\xe8que ",(0,s.jsx)(n.code,{children:"google.genai"})," en Python. Il couvre l'initialisation des sessions de discussion, l'envoi de messages, la surcharge de param\xe8tres pour des requ\xeates individuelles, et, ce qui est important, comment mettre \xe0 jour de mani\xe8re permanente des param\xe8tres tels que ",(0,s.jsx)(n.code,{children:"temperature"})," et ",(0,s.jsx)(n.code,{children:"max_output_tokens"})," tout en pr\xe9servant l'",(0,s.jsx)(n.code,{children:"system_instruction"})," et l'",(0,s.jsx)(n.code,{children:"history"})," existant."]}),"\n",(0,s.jsx)(n.h3,{id:"pr\xe9requis",children:"Pr\xe9requis"}),"\n",(0,s.jsxs)(n.p,{children:["Pour ex\xe9cuter les exemples, assurez-vous que la biblioth\xe8que ",(0,s.jsx)(n.code,{children:"google-generative-ai"})," est install\xe9e. Vous aurez \xe9galement besoin d'une cl\xe9 API pour l'API Gemini ou des identifiants Vertex AI appropri\xe9s. Pour l'API Gemini, vous pouvez d\xe9finir la variable d'environnement ",(0,s.jsx)(n.code,{children:"GOOGLE_API_KEY"})," ou la transmettre directement au constructeur de ",(0,s.jsx)(n.code,{children:"genai.Client"}),"."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install google-generative-ai\n"})}),"\n",(0,s.jsx)(n.h3,{id:"1-initialiser-le-client",children:"1. Initialiser le Client"}),"\n",(0,s.jsxs)(n.p,{children:["Tout d'abord, initialisez l'objet ",(0,s.jsx)(n.code,{children:"Client"}),". C'est le point d'entr\xe9e pour interagir avec les services Google Generative AI."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import google.genai as genai\r\nfrom google.genai import types\r\n\r\n# Configurez votre cl\xe9 API ou vos identifiants Vertex AI\r\n# Option 1 : D\xe9finir la variable d\'environnement GOOGLE_API_KEY\r\n# export GOOGLE_API_KEY="VOTRE_CLE_API"\r\n# client = genai.Client()\r\n\r\n# Option 2 : Transmettre la cl\xe9 API directement (pour l\'API Gemini)\r\nclient = genai.Client(api_key="VOTRE_CLE_API")\r\n\r\n# Option 3 : Pour Vertex AI (n\xe9cessite un projet et une localisation)\r\n# client = genai.Client(vertexai=True, project="votre-id-projet-gcp", location="us-central1")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"2-cr\xe9er-une-conversation-avec-des-param\xe8tres-initiaux",children:"2. Cr\xe9er une conversation avec des param\xe8tres initiaux"}),"\n",(0,s.jsxs)(n.p,{children:["Vous pouvez d\xe9marrer une conversation (session de discussion) avec un historique existant et sp\xe9cifier des param\xe8tres de g\xe9n\xe9ration initiaux tels que ",(0,s.jsx)(n.code,{children:"temperature"}),", ",(0,s.jsx)(n.code,{children:"max_output_tokens"}),", ",(0,s.jsx)(n.code,{children:"top_p"}),", ",(0,s.jsx)(n.code,{children:"top_k"}),", et une ",(0,s.jsx)(n.code,{children:"system_instruction"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["La m\xe9thode ",(0,s.jsx)(n.code,{children:"client.chats.create()"})," est utilis\xe9e pour initialiser un objet ",(0,s.jsx)(n.code,{children:"Chat"}),"."]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"model"}),' : Sp\xe9cifie le mod\xe8le g\xe9n\xe9ratif \xe0 utiliser (par exemple, "gemini-pro").']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"history"})," : Une liste facultative d'objets ",(0,s.jsx)(n.code,{children:"types.Content"})," repr\xe9sentant les tours pr\xe9c\xe9dents de la conversation. Utilisez ",(0,s.jsx)(n.code,{children:"types.UserContent()"})," et ",(0,s.jsx)(n.code,{children:"types.ModelContent()"})," pour une d\xe9finition plus claire de l'historique."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"config"})," : Un objet ",(0,s.jsx)(n.code,{children:"types.GenerateContentConfig"})," facultatif pour d\xe9finir les param\xe8tres de g\xe9n\xe9ration initiaux de la session de discussion. Cette ",(0,s.jsx)(n.code,{children:"config"})," servira de valeur par d\xe9faut pour tous les messages ult\xe9rieurs dans cette session."]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# D\xe9finir un historique de conversation initial facultatif\r\ninitial_history = [\r\n    types.UserContent("Bonjour !"),\r\n    types.ModelContent("Salut ! Comment puis-je vous aider aujourd\'hui ?"),\r\n    types.UserContent("Je souhaite en savoir plus sur les grands mod\xe8les de langage."),\r\n]\r\n\r\n# D\xe9finir les param\xe8tres de configuration de g\xe9n\xe9ration initiaux\r\ninitial_generation_config = types.GenerateContentConfig(\r\n    temperature=0.7,          # Contr\xf4le le caract\xe8re al\xe9atoire : 0.0 (moins al\xe9atoire) \xe0 1.0 (plus al\xe9atoire)\r\n    max_output_tokens=150,    # Nombre maximum de tokens \xe0 g\xe9n\xe9rer dans la r\xe9ponse\r\n    top_p=0.9,                # Les tokens sont \xe9chantillonn\xe9s jusqu\'\xe0 ce que leurs probabilit\xe9s totalisent cette valeur\r\n    top_k=40,                 # Pour chaque \xe9tape, consid\xe9rer les top_k tokens ayant les probabilit\xe9s les plus \xe9lev\xe9es\r\n    system_instruction=types.Content( # Utiliser types.Content pour system_instruction\r\n        parts=[types.Part(text="Vous \xeates un expert en IA et en apprentissage automatique. Fournissez des r\xe9ponses d\xe9taill\xe9es et informatives.")]\r\n    )\r\n)\r\n\r\n# Cr\xe9er la session de discussion\r\nchat = client.chats.create(\r\n    model="gemini-pro", # Utiliser un mod\xe8le appropri\xe9, par exemple "gemini-pro"\r\n    history=initial_history,\r\n    config=initial_generation_config\r\n)\r\n\r\nprint("--- Configuration initiale de la session de discussion ---")\r\nprint(f"Mod\xe8le : {chat._model}")\r\n# Note : L\'acc\xe8s direct \xe0 `_config` (par exemple, `chat._config`) est \xe0 des fins de d\xe9monstration de l\'\xe9tat interne.\r\n# La m\xe9thode `send_message` utilise cette configuration par d\xe9faut en interne.\r\nprint(f"Temp\xe9rature par d\xe9faut : {chat._config.temperature}")\r\nprint(f"Nombre maximal de tokens de sortie par d\xe9faut : {chat._config.max_output_tokens}")\r\nprint(f"Instruction syst\xe8me par d\xe9faut : {chat._config.system_instruction.parts[0].text if chat._config.system_instruction else \'Aucune\'}")\r\n\r\nprint("\\n--- Historique de discussion initial (filtr\xe9) ---")\r\nfor i, content in enumerate(chat.get_history(curated=True)):\r\n    role = content.role\r\n    # V\xe9rifier si content.parts existe et a un attribut text avant d\'y acc\xe9der\r\n    text_content = content.parts[0].text if content.parts and hasattr(content.parts[0], \'text\') else "[Contenu non textuel]"\r\n    print(f"Tour {i+1} ({role}) : {text_content}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"3-envoi-de-messages-et-surcharges-de-param\xe8tres-par-appel",children:"3. Envoi de messages et surcharges de param\xe8tres par appel"}),"\n",(0,s.jsxs)(n.p,{children:["Une fois une conversation initi\xe9e, vous pouvez envoyer de nouveaux messages. La m\xe9thode ",(0,s.jsx)(n.code,{children:"send_message()"})," (ou ",(0,s.jsx)(n.code,{children:"send_message_stream()"})," pour les r\xe9ponses en flux continu) vous permet de surcharger les param\xe8tres de g\xe9n\xe9ration ",(0,s.jsx)(n.strong,{children:"pour cet appel sp\xe9cifique uniquement"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["Vous pouvez modifier des param\xe8tres tels que ",(0,s.jsx)(n.code,{children:"temperature"})," et ",(0,s.jsx)(n.code,{children:"max_output_tokens"})," pour des appels ",(0,s.jsx)(n.code,{children:"send_message"})," individuels en passant un nouvel objet ",(0,s.jsx)(n.code,{children:"types.GenerateContentConfig"})," \xe0 l'argument ",(0,s.jsx)(n.code,{children:"config"}),". Tout param\xe8tre d\xe9fini dans cette ",(0,s.jsx)(n.code,{children:"config"})," par appel aura la priorit\xe9 sur la valeur par d\xe9faut de la session de discussion pour ce message particulier, mais ne ",(0,s.jsx)(n.strong,{children:"modifiera pas"})," la valeur par d\xe9faut de la session pour les messages ult\xe9rieurs."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import time\r\n\r\n# Envoyer un message, en surchargeant la temp\xe9rature et le nombre maximal de tokens de sortie pour ce tour\r\nprint("\\n--- Envoi du Message 1 (surcharge de la config pour cet appel) ---")\r\nmessage_1_config = types.GenerateContentConfig(\r\n    temperature=0.2,       # Temp\xe9rature plus basse pour moins d\'al\xe9atoire sur cet appel\r\n    max_output_tokens=80   # G\xe9n\xe9rer une r\xe9ponse plus courte pour cet appel\r\n)\r\nresponse1 = chat.send_message("Quelles sont les applications des LLM ?", config=message_1_config)\r\nprint(f"Utilisateur : Quelles sont les applications des LLM ?")\r\nprint(f"Mod\xe8le : {response1.text}")\r\nprint(f"Configuration pour le Message 1 (surcharge par appel) : Temp\xe9rature={message_1_config.temperature}, Nombre maximal de tokens de sortie={message_1_config.max_output_tokens}")\r\n\r\n# La configuration par d\xe9faut interne du chat reste inchang\xe9e par l\'appel ci-dessus :\r\n# print(f"Temp\xe9rature par d\xe9faut du chat : {chat._config.temperature}")\r\n\r\n# Envoyer un autre message, en surchargeant avec des param\xe8tres diff\xe9rents\r\nprint("\\n--- Envoi du Message 2 (surcharge \xe0 nouveau de la config pour cet appel) ---")\r\nmessage_2_config = types.GenerateContentConfig(\r\n    temperature=0.9,       # Temp\xe9rature plus \xe9lev\xe9e pour une r\xe9ponse plus cr\xe9ative sur cet appel\r\n    max_output_tokens=120  # Permettre une r\xe9ponse plus longue sur cet appel\r\n)\r\nresponse2 = chat.send_message("Donnez-moi un exemple cr\xe9atif.", config=message_2_config)\r\nprint(f"Utilisateur : Donnez-moi un exemple cr\xe9atif.")\r\nprint(f"Mod\xe8le : {response2.text}")\r\nprint(f"Configuration pour le Message 2 (surcharge par appel) : Temp\xe9rature={message_2_config.temperature}, Nombre maximal de tokens de sortie={message_2_config.max_output_tokens}")\r\n\r\n# Envoyer un message sans surcharger la configuration, ce qui utilisera l\'initial_generation_config\r\nprint("\\n--- Envoi du Message 3 (utilisation de la configuration par d\xe9faut initiale) ---")\r\nresponse3 = chat.send_message("R\xe9sumez notre conversation jusqu\'\xe0 pr\xe9sent.")\r\nprint(f"Utilisateur : R\xe9sumez notre conversation jusqu\'\xe0 pr\xe9sent.")\r\nprint(f"Mod\xe8le : {response3.text}")\r\n# Cet appel utilise la temp\xe9rature et le nombre maximal de tokens de sortie de `initial_generation_config`\r\nprint(f"Configuration pour le Message 3 (par d\xe9faut de la session de discussion) : Temp\xe9rature={initial_generation_config.temperature}, Nombre maximal de tokens de sortie={initial_generation_config.max_output_tokens}")\r\n\r\n# D\xe9montrer la r\xe9ponse en flux continu\r\nprint("\\n--- Envoi du Message 4 (en flux continu, surcharge de la config pour cet appel) ---")\r\nmessage_4_config = types.GenerateContentConfig(\r\n    temperature=0.5,\r\n    max_output_tokens=60\r\n)\r\nprint(f"Utilisateur : Raconte-moi une tr\xe8s courte histoire sur un brave chevalier.")\r\nprint("Mod\xe8le (en flux continu) : ", end="")\r\nfor chunk in chat.send_message_stream("Raconte-moi une tr\xe8s courte histoire sur un brave chevalier.", config=message_4_config):\r\n    print(chunk.text, end="", flush=True)\r\n    time.sleep(0.05) # Simuler l\'impression en temps r\xe9el\r\nprint("\\n")\r\nprint(f"Configuration pour le Message 4 (surcharge par appel) : Temp\xe9rature={message_4_config.temperature}, Nombre maximal de tokens de sortie={message_4_config.max_output_tokens}")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"4-mise-\xe0-jour-des-param\xe8tres-de-discussion-persistants-pr\xe9servation-de-l\xe9tat",children:"4. Mise \xe0 jour des param\xe8tres de discussion persistants (pr\xe9servation de l'\xe9tat)"}),"\n",(0,s.jsxs)(n.p,{children:["La configuration de base d'un objet ",(0,s.jsx)(n.code,{children:"Chat"}),", incluant son ",(0,s.jsx)(n.code,{children:"system_instruction"})," et ses param\xe8tres de g\xe9n\xe9ration par d\xe9faut, est d\xe9finie lors de sa cr\xe9ation et n'est pas directement modifiable via des m\xe9thodes publiques. Pour apporter des modifications \xe0 ",(0,s.jsx)(n.code,{children:"temperature"})," et ",(0,s.jsx)(n.code,{children:"max_output_tokens"})," qui persistent pour tous les messages ",(0,s.jsx)(n.em,{children:"futurs"})," d'une conversation (tout en conservant l'",(0,s.jsx)(n.code,{children:"system_instruction"})," et l'historique de discussion accumul\xe9), vous devez ",(0,s.jsxs)(n.strong,{children:["recr\xe9er l'instance ",(0,s.jsx)(n.code,{children:"Chat"})]}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Ce processus implique :"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"R\xe9cup\xe9ration de l'\xe9tat actuel :"})," Obtenir le nom du mod\xe8le existant, la ",(0,s.jsx)(n.code,{children:"GenerateContentConfig"})," actuelle (qui inclut l'",(0,s.jsx)(n.code,{children:"system_instruction"}),"), et l'historique complet de la conversation \xe0 partir de l'instance ",(0,s.jsx)(n.code,{children:"Chat"})," active."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Collecte des nouveaux param\xe8tres :"})," Obtenir les nouvelles valeurs de ",(0,s.jsx)(n.code,{children:"temperature"})," et ",(0,s.jsx)(n.code,{children:"max_output_tokens"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fusion des configurations :"})," Combiner les valeurs ",(0,s.jsx)(n.code,{children:"GenerateContentConfig"})," existantes avec les nouvelles valeurs de param\xe8tres dans une seule configuration mise \xe0 jour."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:["Recr\xe9ation de l'instance ",(0,s.jsx)(n.code,{children:"Chat"})," :"]})," Cr\xe9er un nouvel objet ",(0,s.jsx)(n.code,{children:"Chat"})," en utilisant ",(0,s.jsx)(n.code,{children:"client.chats.create()"}),", en passant le mod\xe8le original, la configuration nouvellement fusionn\xe9e et l'historique de discussion pr\xe9serv\xe9."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Remplacement de l'ancienne instance :"})," Mettre \xe0 jour la r\xe9f\xe9rence de votre programme \xe0 l'objet ",(0,s.jsx)(n.code,{children:"Chat"})," pour qu'elle pointe vers la nouvelle instance."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Cela assure une transition fluide vers les nouveaux param\xe8tres sans perdre de contexte conversationnel ni d'autres r\xe9glages pr\xe9d\xe9finis."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Fonction pour obtenir de nouveaux param\xe8tres de l\'utilisateur (peut \xeatre int\xe9gr\xe9e \xe0 l\'interface utilisateur de votre application)\r\ndef get_user_param_overrides():\r\n    """Demande \xe0 l\'utilisateur la temp\xe9rature et le nombre maximal de tokens de sortie, permettant une saisie vide pour conserver les valeurs actuelles."""\r\n    print("\\n--- Saisir les nouveaux param\xe8tres du mod\xe8le (Laisser vide pour conserver les valeurs actuelles) ---")\r\n    temp_input = input("Nouvelle temp\xe9rature (flottant, par exemple 0.7) : ")\r\n    max_tokens_input = input("Nouveau nombre maximal de tokens de sortie (entier, par exemple 200) : ")\r\n\r\n    new_temperature = None\r\n    if temp_input:\r\n        try:\r\n            new_temperature = float(temp_input)\r\n            if not (0.0 <= new_temperature <= 1.0):\r\n                print("Note : La temp\xe9rature varie g\xe9n\xe9ralement de 0.0 \xe0 1.0 pour les cas d\'utilisation typiques.")\r\n        except ValueError:\r\n            print("Saisie invalide pour la temp\xe9rature. Conservation de la valeur actuelle.")\r\n\r\n    new_max_output_tokens = None\r\n    if max_tokens_input:\r\n        try:\r\n            new_max_output_tokens = int(max_tokens_input)\r\n            if new_max_output_tokens <= 0:\r\n                print("Note : Le nombre maximal de tokens de sortie doit \xeatre un entier positif.")\r\n        except ValueError:\r\n            print("Saisie invalide pour le nombre maximal de tokens de sortie. Conservation de la valeur actuelle.")\r\n\r\n    return new_temperature, new_max_output_tokens\r\n\r\ndef recreate_chat_with_updates(current_chat: genai.chats.Chat) -> genai.chats.Chat:\r\n    """\r\n    R\xe9cup\xe8re l\'\xe9tat actuel de la discussion, le fusionne avec de nouveaux param\xe8tres et recr\xe9e la discussion.\r\n    Renvoie la nouvelle instance Chat.\r\n    """\r\n    print("\\n--- Mise \xe0 jour permanente des param\xe8tres de discussion ---")\r\n\r\n    # 1. R\xe9cup\xe9rer le nom du mod\xe8le actuel, la configuration et l\'historique\r\n    current_model_name = current_chat._model\r\n    # L\'acc\xe8s direct \xe0 _config est un mod\xe8le courant pour inspecter l\'\xe9tat interne\r\n    # lorsqu\'aucun accesseur public n\'est fourni pour l\'objet de configuration complet.\r\n    current_gen_config: types.GenerateContentConfig = current_chat._config\r\n    current_history = current_chat.get_history(curated=False) # R\xe9cup\xe9rer l\'historique complet\r\n\r\n    # 2. Obtenir les nouvelles valeurs de param\xe8tres de l\'utilisateur ou de la source\r\n    new_temperature, new_max_output_tokens = get_user_param_overrides()\r\n\r\n    # 3. Fusionner les configurations\r\n    # Commencer par une repr\xe9sentation dictionnaire de la configuration actuelle pour conserver tous ses champs\r\n    updated_config_dict = current_gen_config.model_dump()\r\n\r\n    # Appliquer les nouveaux param\xe8tres s\'ils ont \xe9t\xe9 fournis (pas None)\r\n    if new_temperature is not None:\r\n        updated_config_dict[\'temperature\'] = new_temperature\r\n    if new_max_output_tokens is not None:\r\n        updated_config_dict[\'max_output_tokens\'] = new_max_output_tokens\r\n\r\n    # Cr\xe9er un nouvel objet GenerateContentConfig \xe0 partir du dictionnaire fusionn\xe9\r\n    final_gen_config = types.GenerateContentConfig(**updated_config_dict)\r\n\r\n    print("\\nRecr\xe9ation de la session de discussion avec la configuration mise \xe0 jour :")\r\n    print(f"  Mod\xe8le : {current_model_name}")\r\n    print(f"  Nouvelle temp\xe9rature : {final_gen_config.temperature}")\r\n    print(f"  Nouveau nombre maximal de tokens de sortie : {final_gen_config.max_output_tokens}")\r\n    if final_gen_config.system_instruction:\r\n        print(f"  Instruction syst\xe8me : \'{final_gen_config.system_instruction.parts[0].text}\'")\r\n    print(f"  Longueur de l\'historique pr\xe9serv\xe9e : {len(current_history)} tours")\r\n\r\n    # 4. Recr\xe9er la session de discussion avec la configuration et l\'historique combin\xe9s\r\n    updated_chat = client.chats.create(\r\n        model=current_model_name,\r\n        config=final_gen_config,\r\n        history=current_history # Transmettre l\'int\xe9gralit\xe9 de l\'historique de conversation pr\xe9serv\xe9\r\n    )\r\n    return updated_chat\r\n\r\n# --- Boucle de discussion interactive principale pour d\xe9montrer les mises \xe0 jour persistantes ---\r\n# Cette partie est \xe0 des fins de d\xe9monstration et s\'int\xe9grerait dans le flux de votre application.\r\n\r\n# D\xe9finir une session de discussion initiale (comme dans la section 2)\r\ninitial_system_instruction = types.Content(parts=[types.Part.from_text("Vous \xeates un assistant concis. Fournissez des r\xe9ponses courtes.")])\r\ninitial_history = [\r\n    types.UserContent("Qu\'est-ce que Python ?"),\r\n    types.ModelContent("Python est un langage de programmation populaire.")\r\n]\r\ninitial_gen_config = types.GenerateContentConfig(\r\n    temperature=0.4,\r\n    max_output_tokens=50,\r\n    system_instruction=initial_system_instruction\r\n)\r\n\r\nprint("--- Initialisation de la session de discussion principale ---")\r\ncurrent_chat_session = client.chats.create(\r\n    model=\'gemini-pro\',\r\n    config=initial_gen_config,\r\n    history=initial_history\r\n)\r\nprint(f"Configuration actuelle de la discussion : Temp\xe9rature={current_chat_session._config.temperature}, MaxTokens={current_chat_session._config.max_output_tokens}")\r\nprint(f"Instruction syst\xe8me actuelle : {current_chat_session._config.system_instruction.parts[0].text}")\r\nprint(f"Longueur actuelle de l\'historique : {len(current_chat_session.get_history())} tours")\r\n\r\nprint("\\n--- Commencer la discussion ---")\r\nprint("Tapez \'update_params\' pour modifier la temp\xe9rature et le nombre maximal de tokens de sortie de mani\xe8re persistante.")\r\nprint("Tapez \'quit\' ou \'exit\' pour terminer la discussion.")\r\n\r\nwhile True:\r\n    user_input = input("\\nVous : ")\r\n    if user_input.lower() in ["quit", "exit"]:\r\n        print("Fin de la session de discussion. Au revoir !")\r\n        break\r\n\r\n    if user_input.lower() == "update_params":\r\n        # Appeler la fonction pour recr\xe9er la discussion avec les param\xe8tres mis \xe0 jour.\r\n        # La nouvelle instance de discussion renvoy\xe9e remplace l\'ancienne.\r\n        current_chat_session = recreate_chat_with_updates(current_chat_session)\r\n        print("\\nParam\xe8tres de la session de discussion mis \xe0 jour. Continuez \xe0 taper des messages.")\r\n        # Continuer \xe0 la prochaine it\xe9ration de la boucle sans envoyer \'update_params\' au mod\xe8le\r\n        continue\r\n\r\n    try:\r\n        response = current_chat_session.send_message(user_input)\r\n        print(f"Mod\xe8le : {response.text}")\r\n    except Exception as e:\r\n        print(f"Une erreur est survenue lors de la g\xe9n\xe9ration du message : {e}")\r\n        break\r\n\r\nprint("\\n--- Historique de discussion final (complet) ---")\r\n# La propri\xe9t\xe9 history de l\'objet chat garde la trace de tous les tours\r\n# y compris l\'historique original et tous les messages envoy\xe9s et re\xe7us.\r\nfor i, content in enumerate(current_chat_session.get_history()):\r\n    role = content.role\r\n    text_content = content.parts[0].text if content.parts and hasattr(content.parts[0], \'text\') else "[Contenu non textuel]"\r\n    print(f"Tour {i+1} ({role}) : {text_content}")\r\n\r\nprint("\\n--- Historique de discussion final (filtr\xe9) ---")\r\n# L\'historique filtr\xe9 ne contient que les tours valides qui contribuent au contexte du mod\xe8le\r\nfor i, content in enumerate(current_chat_session.get_history(curated=True)):\r\n    role = content.role\r\n    text_content = content.parts[0].text if content.parts and hasattr(content.parts[0], \'text\') else "[Contenu non textuel]"\r\n    print(f"Tour {i+1} ({role}) : {text_content}")\n'})})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var r=t(6540);const s={},i=r.createContext(s);function o(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);