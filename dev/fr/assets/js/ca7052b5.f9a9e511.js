"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[6452],{6421:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Python/Openai/Api/open-api-code-examples","title":"Compl\xe9tion de chat OpenAI","description":"Le cas d\'utilisation le plus courant de l\'API OpenAI en Python est la g\xe9n\xe9ration de compl\xe9tions de chat. Cela implique d\'envoyer une liste de messages \xe0 un mod\xe8le sp\xe9cifi\xe9 et de recevoir une r\xe9ponse g\xe9n\xe9r\xe9e. L\'API prend en charge les appels synchrones (bloquants) et asynchrones (non bloquants), et les r\xe9ponses peuvent \xeatre re\xe7ues sous forme d\'objet unique ou \xeatre diffus\xe9es par fragments.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/Python/Openai/Api/open-api-code-examples.md","sourceDirName":"Python/Openai/Api","slug":"/Python/Openai/Api/open-api-code-examples","permalink":"/dev/fr/docs/Python/Openai/Api/open-api-code-examples","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"python","permalink":"/dev/fr/docs/tags/python"},{"inline":true,"label":"openai","permalink":"/dev/fr/docs/tags/openai"},{"inline":true,"label":"chat","permalink":"/dev/fr/docs/tags/chat"},{"inline":true,"label":"api","permalink":"/dev/fr/docs/tags/api"}],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Compl\xe9tion de chat OpenAI","sidebar_position":2,"tags":["python","openai","chat","api"]},"sidebar":"tutorialSidebar","previous":{"title":"Utilisations de l\'API OpenAI","permalink":"/dev/fr/docs/Python/Openai/Api/openai-api-intro"},"next":{"title":"Exemples d\'API OpenAI","permalink":"/dev/fr/docs/Python/Openai/Api/openai-use-case"}}');var o=r(4848),s=r(8453);const i={title:"Compl\xe9tion de chat OpenAI",sidebar_position:2,tags:["python","openai","chat","api"]},a=void 0,c={},l=[{value:"Compl\xe9tion de chat synchrone (sans diffusion)",id:"compl\xe9tion-de-chat-synchrone-sans-diffusion",level:3},{value:"Compl\xe9tion de chat synchrone (avec diffusion)",id:"compl\xe9tion-de-chat-synchrone-avec-diffusion",level:3},{value:"Compl\xe9tion de chat asynchrone (sans diffusion)",id:"compl\xe9tion-de-chat-asynchrone-sans-diffusion",level:3},{value:"Compl\xe9tion de chat asynchrone (avec diffusion)",id:"compl\xe9tion-de-chat-asynchrone-avec-diffusion",level:3}];function p(e){const n={code:"code",h3:"h3",p:"p",pre:"pre",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"Le cas d'utilisation le plus courant de l'API OpenAI en Python est la g\xe9n\xe9ration de compl\xe9tions de chat. Cela implique d'envoyer une liste de messages \xe0 un mod\xe8le sp\xe9cifi\xe9 et de recevoir une r\xe9ponse g\xe9n\xe9r\xe9e. L'API prend en charge les appels synchrones (bloquants) et asynchrones (non bloquants), et les r\xe9ponses peuvent \xeatre re\xe7ues sous forme d'objet unique ou \xeatre diffus\xe9es par fragments."}),"\n",(0,o.jsx)(n.h3,{id:"compl\xe9tion-de-chat-synchrone-sans-diffusion",children:"Compl\xe9tion de chat synchrone (sans diffusion)"}),"\n",(0,o.jsx)(n.p,{children:"Cet exemple montre comment envoyer une seule invite \xe0 l'API OpenAI et recevoir la r\xe9ponse compl\xe8te en une seule fois."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\r\n\r\n# Initialise le client OpenAI.\r\n# La cl\xe9 API est automatiquement charg\xe9e depuis la variable d\'environnement OPENAI_API_KEY.\r\n# Alternativement, vous pouvez la passer directement : client = OpenAI(api_key="votre_cl\xe9_api_ici")\r\nclient = OpenAI()\r\n\r\ntry:\r\n    # Cr\xe9e une compl\xe9tion de chat\r\n    chat_completion = client.chat.completions.create(\r\n        messages=[\r\n            {\r\n                "role": "user",\r\n                "content": "Quelle est la capitale de la France ?",\r\n            }\r\n        ],\r\n        model="gpt-4o", # Sp\xe9cifie le mod\xe8le \xe0 utiliser\r\n    )\r\n\r\n    # Acc\xe8de au contenu du message g\xe9n\xe9r\xe9\r\n    response_content = chat_completion.choices[0].message.content\r\n    print(f"R\xe9ponse sans diffusion :\\n{response_content}")\r\n\r\nexcept Exception as e:\r\n    print(f"Une erreur est survenue : {e}")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"compl\xe9tion-de-chat-synchrone-avec-diffusion",children:"Compl\xe9tion de chat synchrone (avec diffusion)"}),"\n",(0,o.jsx)(n.p,{children:"Cet exemple montre comment recevoir la r\xe9ponse du mod\xe8le de mani\xe8re incr\xe9mentielle au fur et \xe0 mesure qu'elle est g\xe9n\xe9r\xe9e, ce qui est utile pour les applications qui souhaitent afficher la r\xe9ponse en temps r\xe9el."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'from openai import OpenAI\r\n\r\nclient = OpenAI()\r\n\r\nprint("R\xe9ponse avec diffusion :")\r\ntry:\r\n    # Cr\xe9e une compl\xe9tion de chat avec diffusion activ\xe9e\r\n    stream = client.chat.completions.create(\r\n        messages=[\r\n            {\r\n                "role": "user",\r\n                "content": "\xc9cris un court po\xe8me inspirant sur la beaut\xe9 de la nature.",\r\n            }\r\n        ],\r\n        model="gpt-4o", # Sp\xe9cifie le mod\xe8le \xe0 utiliser\r\n        stream=True, # Active la diffusion\r\n    )\r\n\r\n    # It\xe8re sur le flux pour afficher les fragments au fur et \xe0 mesure de leur arriv\xe9e\r\n    for chunk in stream:\r\n        # Chaque fragment contient un objet \'delta\' avec un contenu partiel\r\n        # Utilise .delta.content ou "" pour g\xe9rer les cas o\xf9 le contenu pourrait \xeatre None\r\n        print(chunk.choices[0].delta.content or "", end="")\r\n    print("\\n") # Ajoute une nouvelle ligne \xe0 la fin pour une sortie propre\r\n\r\nexcept Exception as e:\r\n    print(f"Une erreur est survenue : {e}")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"compl\xe9tion-de-chat-asynchrone-sans-diffusion",children:"Compl\xe9tion de chat asynchrone (sans diffusion)"}),"\n",(0,o.jsxs)(n.p,{children:["Pour les applications asynchrones, le client ",(0,o.jsx)(n.code,{children:"AsyncOpenAI"})," peut \xeatre utilis\xe9 avec ",(0,o.jsx)(n.code,{children:"await"}),"."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import asyncio\r\nfrom openai import AsyncOpenAI\r\n\r\nasync def async_non_streaming_completion():\r\n    # Initialise le client OpenAI asynchrone\r\n    client = AsyncOpenAI()\r\n\r\n    try:\r\n        # Cr\xe9e une compl\xe9tion de chat asynchrone\r\n        chat_completion = await client.chat.completions.create(\r\n            messages=[\r\n                {\r\n                    "role": "user",\r\n                    "content": "R\xe9sume l\'id\xe9e principale de la physique quantique en une phrase.",\r\n                }\r\n            ],\r\n            model="gpt-4o",\r\n        )\r\n\r\n        response_content = chat_completion.choices[0].message.content\r\n        print(f"R\xe9ponse asynchrone sans diffusion :\\n{response_content}")\r\n\r\n    except Exception as e:\r\n        print(f"Une erreur est survenue : {e}")\r\n\r\n# Ex\xe9cute la fonction asynchrone\r\n# asyncio.run(async_non_streaming_completion())\n'})}),"\n",(0,o.jsx)(n.h3,{id:"compl\xe9tion-de-chat-asynchrone-avec-diffusion",children:"Compl\xe9tion de chat asynchrone (avec diffusion)"}),"\n",(0,o.jsx)(n.p,{children:"Les r\xe9ponses avec diffusion sont \xe9galement disponibles de mani\xe8re asynchrone, permettant des mises \xe0 jour efficaces en temps r\xe9el dans les applications asynchrones."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import asyncio\r\nfrom openai import AsyncOpenAI\r\n\r\nasync def async_streaming_completion():\r\n    client = AsyncOpenAI()\r\n\r\n    print("R\xe9ponse asynchrone avec diffusion :")\r\n    try:\r\n        # Cr\xe9e une compl\xe9tion de chat asynchrone avec diffusion\r\n        stream = await client.chat.completions.create(\r\n            messages=[\r\n                {\r\n                    "role": "user",\r\n                    "content": "Raconte-moi une br\xe8ve histoire sur un chevalier courageux et un dragon.",\r\n                }\r\n            ],\r\n            model="gpt-4o",\r\n            stream=True,\r\n        )\r\n\r\n        async for chunk in stream:\r\n            print(chunk.choices[0].delta.content or "", end="")\r\n        print("\\n")\r\n\r\n    except Exception as e:\r\n        print(f"Une erreur est survenue : {e}")\r\n\r\n# Ex\xe9cute la fonction asynchrone\r\n# asyncio.run(async_streaming_completion())\n'})})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>a});var t=r(6540);const o={},s=t.createContext(o);function i(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:i(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);