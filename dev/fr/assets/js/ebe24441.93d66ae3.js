"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[6190],{6600:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"Python/Gemini/Api/preserve-chat-config","title":"Pr\xe9server les mises \xe0 jour de la configuration du chat","description":"Lors de l\'utilisation de la fonctionnalit\xe9 de chat de l\'API Gemini, il est courant d\'initialiser une session de chat avec des param\xe8tres sp\xe9cifiques tels que la temperature, maxoutputtokens, systeminstruction et un history initial. Un d\xe9fi se pose lorsque vous souhaitez ensuite mettre \xe0 jour dynamiquement uniquement la temperature et maxoutputtokens sans perdre l\'systeminstruction pr\xe9c\xe9demment d\xe9fini ou l\'history de conversation accumul\xe9.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/Python/Gemini/Api/preserve-chat-config.md","sourceDirName":"Python/Gemini/Api","slug":"/Python/Gemini/Api/preserve-chat-config","permalink":"/dev/fr/docs/Python/Gemini/Api/preserve-chat-config","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"API Gemini","permalink":"/dev/fr/docs/tags/api-gemini"},{"inline":true,"label":"Gestion de session de chat","permalink":"/dev/fr/docs/tags/gestion-de-session-de-chat"},{"inline":true,"label":"Configuration dynamique","permalink":"/dev/fr/docs/tags/configuration-dynamique"},{"inline":true,"label":"Instruction syst\xe8me","permalink":"/dev/fr/docs/tags/instruction-systeme"},{"inline":true,"label":"Historique du chat","permalink":"/dev/fr/docs/tags/historique-du-chat"},{"inline":true,"label":"Param\xe8tres du mod\xe8le","permalink":"/dev/fr/docs/tags/parametres-du-modele"}],"version":"current","sidebarPosition":3,"frontMatter":{"title":"Pr\xe9server les mises \xe0 jour de la configuration du chat","sidebar_position":3,"tags":["API Gemini","Gestion de session de chat","Configuration dynamique","Instruction syst\xe8me","Historique du chat","Param\xe8tres du mod\xe8le"]},"sidebar":"tutorialSidebar","previous":{"title":"Conventions de nommage","permalink":"/dev/fr/docs/Python/python-naming-convention"},"next":{"title":"chat-api","permalink":"/dev/fr/docs/Python/Gemini/Api/chat-api"}}');var i=t(4848),s=t(8453);const o={title:"Pr\xe9server les mises \xe0 jour de la configuration du chat",sidebar_position:3,tags:["API Gemini","Gestion de session de chat","Configuration dynamique","Instruction syst\xe8me","Historique du chat","Param\xe8tres du mod\xe8le"]},a=void 0,c={},l=[{value:"Composants cl\xe9s",id:"composants-cl\xe9s",level:3},{value:"Guide \xe9tape par \xe9tape pour la pr\xe9servation de la configuration",id:"guide-\xe9tape-par-\xe9tape-pour-la-pr\xe9servation-de-la-configuration",level:3},{value:"Exemple de code",id:"exemple-de-code",level:3}];function u(e){const n={code:"code",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["Lors de l'utilisation de la fonctionnalit\xe9 de chat de l'API Gemini, il est courant d'initialiser une session de chat avec des param\xe8tres sp\xe9cifiques tels que la ",(0,i.jsx)(n.code,{children:"temperature"}),", ",(0,i.jsx)(n.code,{children:"max_output_tokens"}),", ",(0,i.jsx)(n.code,{children:"system_instruction"})," et un ",(0,i.jsx)(n.code,{children:"history"})," initial. Un d\xe9fi se pose lorsque vous souhaitez ensuite mettre \xe0 jour dynamiquement uniquement la ",(0,i.jsx)(n.code,{children:"temperature"})," et ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," sans perdre l'",(0,i.jsx)(n.code,{children:"system_instruction"})," pr\xe9c\xe9demment d\xe9fini ou l'",(0,i.jsx)(n.code,{children:"history"})," de conversation accumul\xe9."]}),"\n",(0,i.jsxs)(n.p,{children:["Le principe fondamental \xe0 comprendre de la structure d'API fournie est que la configuration (",(0,i.jsx)(n.code,{children:"_config"}),") et l'historique (",(0,i.jsx)(n.code,{children:"_comprehensive_history"}),") d'un objet ",(0,i.jsx)(n.code,{children:"Chat"})," sont d\xe9finis lors de son initialisation (",(0,i.jsx)(n.code,{children:"__init__"}),") et ne sont pas directement modifiables via des m\xe9thodes publiques par la suite. Par cons\xe9quent, pour r\xe9aliser des mises \xe0 jour dynamiques tout en pr\xe9servant tous les param\xe8tres existants, vous devez :"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"R\xe9cup\xe9rer"})," l'\xe9tat actuel de la session ",(0,i.jsx)(n.code,{children:"Chat"})," existante (sa configuration et son historique)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fusionner"})," les nouvelles valeurs de ",(0,i.jsx)(n.code,{children:"temperature"})," et ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," avec la configuration r\xe9cup\xe9r\xe9e."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recr\xe9er"})," une nouvelle instance ",(0,i.jsx)(n.code,{children:"Chat"})," en utilisant la configuration combin\xe9e et mise \xe0 jour, et l'historique pr\xe9serv\xe9."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"composants-cl\xe9s",children:"Composants cl\xe9s"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"google.genai.Client"})," : Le client principal pour interagir avec l'API Gemini."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"google.genai.chats.Chat"})," : Repr\xe9sente une session de chat en cours."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"google.genai.types.GenerateContentConfig"})," : Un mod\xe8le Pydantic (ou ",(0,i.jsx)(n.code,{children:"GenerateContentConfigDict"})," pour une entr\xe9e de dictionnaire) qui encapsule divers param\xe8tres de g\xe9n\xe9ration de mod\xe8le, y compris ",(0,i.jsx)(n.code,{children:"temperature"}),", ",(0,i.jsx)(n.code,{children:"max_output_tokens"}),", et ",(0,i.jsx)(n.code,{children:"system_instruction"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"google.genai.types.Content"})," : Repr\xe9sente un \xe9l\xe9ment de contenu dans la conversation, utilis\xe9 \xe0 la fois pour ",(0,i.jsx)(n.code,{children:"system_instruction"})," et ",(0,i.jsx)(n.code,{children:"history"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"guide-\xe9tape-par-\xe9tape-pour-la-pr\xe9servation-de-la-configuration",children:"Guide \xe9tape par \xe9tape pour la pr\xe9servation de la configuration"}),"\n",(0,i.jsxs)(n.p,{children:["Supposons que vous ayez un objet ",(0,i.jsx)(n.code,{children:"chat"})," existant qui a \xe9t\xe9 cr\xe9\xe9 avec une ",(0,i.jsx)(n.code,{children:"system_instruction"})," initiale et qui a accumul\xe9 de l'",(0,i.jsx)(n.code,{children:"history"}),"."]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Acc\xe9der \xe0 la configuration et \xe0 l'historique actuels :"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["La ",(0,i.jsx)(n.code,{children:"GenerateContentConfig"})," actuelle est stock\xe9e dans l'attribut ",(0,i.jsx)(n.code,{children:"_config"})," de l'objet ",(0,i.jsx)(n.code,{children:"Chat"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["L'historique actuel du chat est accessible via la m\xe9thode ",(0,i.jsx)(n.code,{children:"get_history()"})," de l'objet ",(0,i.jsx)(n.code,{children:"Chat"}),". Il est recommand\xe9 de r\xe9cup\xe9rer l'historique ",(0,i.jsx)(n.code,{children:"comprehensive"})," (par d\xe9faut ",(0,i.jsx)(n.code,{children:"curated=False"}),") pour s'assurer que toutes les interactions, y compris celles avec des sorties invalides, sont pr\xe9serv\xe9es pour le contexte."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# \xc0 partir d'un objet chat_session existant\r\ncurrent_config = chat_session._config\r\ncurrent_history = chat_session.get_history(curated=False)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pr\xe9parer les nouvelles valeurs de param\xe8tres :"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Obtenez les nouvelles ",(0,i.jsx)(n.code,{children:"temperature"})," et ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," souhait\xe9es de l'utilisateur ou d'une autre source."]}),"\n",(0,i.jsx)(n.li,{children:"Impl\xe9mentez une logique pour g\xe9rer les cas o\xf9 l'utilisateur ne souhaiterait pas modifier un param\xe8tre sp\xe9cifique (par exemple, autoriser une entr\xe9e vide pour conserver la valeur actuelle)."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Fusionner les configurations :"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Convertissez la ",(0,i.jsx)(n.code,{children:"current_config"})," (qui est un mod\xe8le Pydantic) en un dictionnaire mutable en utilisant ",(0,i.jsx)(n.code,{children:".model_dump()"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Mettez \xe0 jour ce dictionnaire avec les nouvelles valeurs de ",(0,i.jsx)(n.code,{children:"temperature"})," et ",(0,i.jsx)(n.code,{children:"max_output_tokens"}),". L'",(0,i.jsx)(n.code,{children:"system_instruction"})," existante (et tout autre champ ",(0,i.jsx)(n.code,{children:"GenerateContentConfig"}),") sera automatiquement report\xe9e de ",(0,i.jsx)(n.code,{children:"current_config"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Reconstruisez un nouvel objet ",(0,i.jsx)(n.code,{children:"types.GenerateContentConfig"})," \xe0 partir de ce dictionnaire fusionn\xe9."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# new_temperature et new_max_output_tokens sont obtenus de l'entr\xe9e utilisateur\r\n# (par exemple, None si l'utilisateur n'a pas sp\xe9cifi\xe9 de changement)\r\n\r\n# Convertir la configuration actuelle en dictionnaire pour la fusion\r\nnew_config_dict = current_config.model_dump() if current_config else {}\r\n\r\nif new_temperature is not None:\r\n    new_config_dict['temperature'] = new_temperature\r\nif new_max_output_tokens is not None:\r\n    new_config_dict['max_output_tokens'] = new_max_output_tokens\r\n\r\n# Cr\xe9er l'objet GenerateContentConfig final\r\nfinal_config = types.GenerateContentConfig(**new_config_dict)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Recr\xe9er la session de chat :"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Appelez ",(0,i.jsx)(n.code,{children:"client.chats.create()"})," \xe0 nouveau."]}),"\n",(0,i.jsxs)(n.li,{children:["Passez le nom du ",(0,i.jsx)(n.code,{children:"model"})," original (",(0,i.jsx)(n.code,{children:"chat_session._model"}),")."]}),"\n",(0,i.jsxs)(n.li,{children:["Passez la ",(0,i.jsx)(n.code,{children:"final_config"})," qui contient maintenant tous les param\xe8tres pr\xe9serv\xe9s et mis \xe0 jour."]}),"\n",(0,i.jsxs)(n.li,{children:["Passez l'",(0,i.jsx)(n.code,{children:"current_history"})," pour vous assurer que le contexte conversationnel est maintenu."]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"updated_chat_session = client.chats.create(\r\n    model=chat_session._model,\r\n    config=final_config,\r\n    history=current_history # Passer l'historique pr\xe9serv\xe9\r\n)\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Remplacer l'ancienne instance de chat :"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Il est crucial de remplacer votre r\xe9f\xe9rence \xe0 l'ancien objet ",(0,i.jsx)(n.code,{children:"chat_session"})," par l'objet ",(0,i.jsx)(n.code,{children:"updated_chat_session"}),". Toutes les interactions ult\xe9rieures utiliseront alors la nouvelle instance ",(0,i.jsx)(n.code,{children:"Chat"})," avec les param\xe8tres persistants souhait\xe9s."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exemple-de-code",children:"Exemple de code"}),"\n",(0,i.jsxs)(n.p,{children:["L'exemple suivant d\xe9montre un chat interactif o\xf9 les utilisateurs peuvent taper ",(0,i.jsx)(n.code,{children:"update_params"})," pour modifier la ",(0,i.jsx)(n.code,{children:"temperature"})," et ",(0,i.jsx)(n.code,{children:"max_output_tokens"})," tout en conservant l'",(0,i.jsx)(n.code,{children:"system_instruction"})," et l'",(0,i.jsx)(n.code,{children:"history"})," intacts."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import google.genai as genai\r\nfrom google.genai import types\r\nimport os\r\n\r\n# Assurez-vous que votre cl\xe9 API est d\xe9finie comme variable d\'environnement ou transmise directement\r\n# os.environ["GOOGLE_API_KEY"] = "VOTRE_CLE_API"\r\n\r\n# --- Initialisation du client ---\r\ntry:\r\n    client = genai.Client()\r\nexcept Exception:\r\n    print("Le client Google GenAI n\'a pas pu \xeatre initialis\xe9.")\r\n    print("Veuillez vous assurer que la variable d\'environnement GOOGLE_API_KEY est d\xe9finie ou la transmettre directement.")\r\n    print("Utilisation d\'un client factice \xe0 des fins de d\xe9monstration uniquement si un client r\xe9el n\'est pas configur\xe9.")\r\n\r\n    # --- Client factice pour les tests locaux sans cl\xe9 API ---\r\n    class MockResponse:\r\n        def __init__(self, text):\r\n            self.text = text\r\n        def __str__(self):\r\n            return self.text\r\n\r\n    class MockChat:\r\n        def __init__(self, model: str, config: types.GenerateContentConfig, history: list[types.Content]):\r\n            self._model = model\r\n            self._config = config\r\n            self._comprehensive_history = history if history is not None else []\r\n            print(f"MockChat cr\xe9\xe9 : Mod\xe8le=\'{model}\', Temp={config.temperature}, MaxTokens={config.max_output_tokens}, Longueur de l\'historique={len(self._comprehensive_history)}")\r\n            if config.system_instruction:\r\n                print(f"  Instruction syst\xe8me : \'{config.system_instruction.parts[0].text}\'")\r\n\r\n        def send_message(self, message: str):\r\n            # Ajouter le message utilisateur \xe0 l\'historique factice\r\n            self._comprehensive_history.append(types.Content(role=\'user\', parts=[types.Part.from_text(message)]))\r\n\r\n            # Simuler la r\xe9ponse du mod\xe8le\r\n            response_text = (\r\n                f"R\xe9ponse factice (T={self._config.temperature}, MaxTok={self._config.max_output_tokens})"\r\n                f" \xe0 : \'{message}\'. Instruction syst\xe8me actuelle : \'{self._config.system_instruction.parts[0].text}\'"\r\n            )\r\n            model_content = types.Content(role=\'model\', parts=[types.Part.from_text(response_text)])\r\n            self._comprehensive_history.append(model_content)\r\n\r\n            # Retourner une r\xe9ponse factice GenerateContentResponse\r\n            mock_gen_response = types.GenerateContentResponse(\r\n                candidates=[types.Candidate(content=model_content)]\r\n            )\r\n            return mock_gen_response\r\n\r\n        def get_history(self, curated: bool = False) -> list[types.Content]:\r\n            # Pour le factice, toujours retourner l\'historique complet\r\n            return self._comprehensive_history\r\n\r\n    class MockChats:\r\n        def create(self, model: str, config: types.GenerateContentConfig, history: list[types.Content] = None):\r\n            return MockChat(model=model, config=config, history=history)\r\n\r\n    class MockClient:\r\n        def __init__(self):\r\n            self.chats = MockChats()\r\n    client = MockClient()\r\n# --- Fin du client factice ---\r\n\r\ndef get_user_param_overrides():\r\n    """Demande \xe0 l\'utilisateur la temp\xe9rature et le nombre maximal de tokens de sortie, en autorisant une entr\xe9e vide."""\r\n    print("\\n--- Entrez les nouveaux param\xe8tres du mod\xe8le (Laissez vide pour conserver les valeurs actuelles) ---")\r\n    temp_input = input("Nouvelle temp\xe9rature (flottant, ex. 0.7) : ")\r\n    max_tokens_input = input("Nouveau nombre maximal de tokens de sortie (entier, ex. 200) : ")\r\n\r\n    new_temperature = None\r\n    if temp_input:\r\n        try:\r\n            new_temperature = float(temp_input)\r\n            if not (0.0 <= new_temperature <= 1.0):\r\n                print("Remarque : La temp\xe9rature est g\xe9n\xe9ralement comprise entre 0.0 et 1.0 pour les cas d\'utilisation typiques.")\r\n        except ValueError:\r\n            print("Entr\xe9e invalide pour la temp\xe9rature. Conservation de la valeur actuelle.")\r\n            new_temperature = None # R\xe9initialiser \xe0 None si invalide\r\n\r\n    new_max_output_tokens = None\r\n    if max_tokens_input:\r\n        try:\r\n            new_max_output_tokens = int(max_tokens_input)\r\n            if new_max_output_tokens <= 0:\r\n                print("Remarque : Le nombre maximal de tokens de sortie doit \xeatre un entier positif.")\r\n                new_max_output_tokens = None # R\xe9initialiser \xe0 None si invalide\r\n        except ValueError:\r\n            print("Entr\xe9e invalide pour le nombre maximal de tokens de sortie. Conservation de la valeur actuelle.")\r\n            new_max_output_tokens = None # R\xe9initialiser \xe0 None si invalide\r\n\r\n    return new_temperature, new_max_output_tokens\r\n\r\ndef recreate_chat_with_updates(current_chat: genai.chats.Chat) -> genai.chats.Chat:\r\n    """\r\n    R\xe9cup\xe8re l\'\xe9tat actuel du chat, le fusionne avec de nouveaux param\xe8tres et recr\xe9e le chat.\r\n    """\r\n    print("\\nTentative de mise \xe0 jour des param\xe8tres du chat...")\r\n\r\n    # 1. R\xe9cup\xe9rer la configuration et l\'historique actuels\r\n    # L\'acc\xe8s direct \xe0 _config est n\xe9cessaire car il n\'y a pas de getter public\r\n    current_model_name = current_chat._model\r\n    current_gen_config: types.GenerateContentConfig = current_chat._config\r\n    current_history = current_chat.get_history(curated=False) # Obtenir l\'historique complet\r\n\r\n    # 2. Obtenir les nouvelles valeurs de param\xe8tres de l\'utilisateur\r\n    new_temperature, new_max_output_tokens = get_user_param_overrides()\r\n\r\n    # 3. Fusionner les configurations\r\n    # Commencer par une repr\xe9sentation dictionnaire de la configuration actuelle\r\n    updated_config_dict = current_gen_config.model_dump()\r\n\r\n    # Appliquer les nouveaux param\xe8tres s\'ils sont fournis par l\'utilisateur\r\n    if new_temperature is not None:\r\n        updated_config_dict[\'temperature\'] = new_temperature\r\n    if new_max_output_tokens is not None:\r\n        updated_config_dict[\'max_output_tokens\'] = new_max_output_tokens\r\n\r\n    # Cr\xe9er un nouvel objet GenerateContentConfig \xe0 partir du dictionnaire fusionn\xe9\r\n    final_gen_config = types.GenerateContentConfig(**updated_config_dict)\r\n\r\n    print("\\nRecr\xe9ation de la session de chat avec :")\r\n    print(f"  Mod\xe8le : {current_model_name}")\r\n    print(f"  Temp\xe9rature : {final_gen_config.temperature}")\r\n    print(f"  Tokens de sortie max. : {final_gen_config.max_output_tokens}")\r\n    if final_gen_config.system_instruction:\r\n        # L\'acc\xe8s \xe0 parts[0].text est une simplification pour les instructions syst\xe8me textuelles typiques\r\n        print(f"  Instruction syst\xe8me : \'{final_gen_config.system_instruction.parts[0].text}\'")\r\n    print(f"  Longueur de l\'historique : {len(current_history)} tours")\r\n\r\n    # 4. Recr\xe9er la session de chat\r\n    updated_chat = client.chats.create(\r\n        model=current_model_name,\r\n        config=final_gen_config,\r\n        history=current_history # Passer l\'historique pr\xe9serv\xe9\r\n    )\r\n    return updated_chat\r\n\r\ndef main_interactive_chat():\r\n    """Initialise et ex\xe9cute une session de chat interactive."""\r\n\r\n    # --- Configuration initiale du chat ---\r\n    # D\xe9finir une instruction syst\xe8me et un historique initiaux\r\n    initial_system_instruction = types.Content(parts=[types.Part.from_text("Vous \xeates un assistant IA comp\xe9tent et amical. Gardez les r\xe9ponses concises sauf si on vous demande des d\xe9tails.")])\r\n    initial_history = [\r\n        types.Content(role=\'user\', parts=[types.Part.from_text("Salut, quel est ton but ?")]),\r\n        types.Content(role=\'model\', parts=[types.Part.from_text("Je suis l\xe0 pour vous aider avec des informations et des t\xe2ches, con\xe7u par Google.")])\r\n    ]\r\n\r\n    # D\xe9finir la configuration de g\xe9n\xe9ration initiale\r\n    initial_gen_config = types.GenerateContentConfig(\r\n        temperature=0.7,\r\n        max_output_tokens=150,\r\n        system_instruction=initial_system_instruction\r\n    )\r\n\r\n    print("--- Initialisation de la session de chat ---")\r\n    current_chat_session = client.chats.create(\r\n        model=\'gemini-pro\', # Ou votre mod\xe8le pr\xe9f\xe9r\xe9\r\n        config=initial_gen_config,\r\n        history=initial_history\r\n    )\r\n    print(f"Configuration initiale du chat : Temp\xe9rature={current_chat_session._config.temperature}, Tokens max.={current_chat_session._config.max_output_tokens}")\r\n    print(f"Instruction syst\xe8me initiale : {current_chat_session._config.system_instruction.parts[0].text}")\r\n    print(f"Longueur de l\'historique initial : {len(current_chat_session.get_history())} tours")\r\n\r\n\r\n    print("\\n--- D\xe9but du chat ---")\r\n    print("Tapez \'update_params\' pour modifier la temp\xe9rature et le nombre maximal de tokens de sortie.")\r\n    print("Tapez \'quit\' ou \'exit\' pour terminer le chat.")\r\n\r\n    while True:\r\n        user_input = input("\\nVous : ")\r\n        if user_input.lower() in ["quit", "exit"]:\r\n            print("Fin de la session de chat. Au revoir !")\r\n            break\r\n\r\n        if user_input.lower() == "update_params":\r\n            # Appeler la fonction pour recr\xe9er le chat avec les param\xe8tres mis \xe0 jour\r\n            current_chat_session = recreate_chat_with_updates(current_chat_session)\r\n            print("\\nParam\xe8tres de la session de chat mis \xe0 jour. Continuez \xe0 taper des messages.")\r\n            continue # Ignorer l\'envoi du message "update_params" au mod\xe8le\r\n\r\n        try:\r\n            response = current_chat_session.send_message(user_input)\r\n            print(f"Mod\xe8le : {response.text}")\r\n        except Exception as e:\r\n            print(f"Une erreur est survenue pendant la g\xe9n\xe9ration : {e}")\r\n            break\r\n\r\nif __name__ == "__main__":\r\n    main_interactive_chat()\n'})})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(u,{...e})}):u(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>a});var r=t(6540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);