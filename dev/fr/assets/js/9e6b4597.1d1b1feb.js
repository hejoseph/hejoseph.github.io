"use strict";(self.webpackChunkmydevdocs=self.webpackChunkmydevdocs||[]).push([[7232],{3589:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>a,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"tech-related/Chatbot/optimize-chat-memory-limit-token-reached","title":"Gestion des Limites de Tokens","description":"Lors de l\u2019utilisation d\u2019API de mod\xe8les de langage (LLM) comme OpenAI GPT-4, chaque appel API est soumis \xe0 une limite maximale de tokens (par exemple, 8192 ou 128k tokens). Les conversations longues d\xe9passent facilement cette limite, surtout si l\u2019on conserve l\u2019historique pour assurer la continuit\xe9.","source":"@site/i18n/fr/docusaurus-plugin-content-docs/current/tech-related/Chatbot/optimize-chat-memory-limit-token-reached.md","sourceDirName":"tech-related/Chatbot","slug":"/tech-related/Chatbot/optimize-chat-memory-limit-token-reached","permalink":"/dev/fr/docs/tech-related/Chatbot/optimize-chat-memory-limit-token-reached","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"LLM","permalink":"/dev/fr/docs/tags/llm"},{"inline":true,"label":"gestion de contexte","permalink":"/dev/fr/docs/tags/gestion-de-contexte"},{"inline":true,"label":"optimisation de conversations","permalink":"/dev/fr/docs/tags/optimisation-de-conversations"}],"version":"current","sidebarPosition":6,"frontMatter":{"title":"Gestion des Limites de Tokens","sidebar_position":6,"tags":["LLM","gestion de contexte","optimisation de conversations"]},"sidebar":"tutorialSidebar","previous":{"title":"D\xe9compte de jetons dans l\u2019IA","permalink":"/dev/fr/docs/tech-related/Chatbot/understand-models-token"},"next":{"title":"Max Token","permalink":"/dev/fr/docs/tech-related/Chatbot/understand-models-max-token"}}');var i=n(4848),r=n(8453);const l={title:"Gestion des Limites de Tokens",sidebar_position:6,tags:["LLM","gestion de contexte","optimisation de conversations"]},o="Gestion des Limites de Tokens dans les Conversations IA",a={},d=[{value:"Pourquoi les Limites de Tokens Sont Importantes",id:"pourquoi-les-limites-de-tokens-sont-importantes",level:2},{value:"Strat\xe9gies pour G\xe9rer les Limites de Tokens",id:"strat\xe9gies-pour-g\xe9rer-les-limites-de-tokens",level:2},{value:"1. Tronquer les Anciens Messages",id:"1-tronquer-les-anciens-messages",level:3},{value:"2. R\xe9sumer les Anciens Messages",id:"2-r\xe9sumer-les-anciens-messages",level:3},{value:"3. Strat\xe9gie Hybride (Recommand\xe9e)",id:"3-strat\xe9gie-hybride-recommand\xe9e",level:3},{value:"4. M\xe9moire Augment\xe9e par Recherche (Avanc\xe9)",id:"4-m\xe9moire-augment\xe9e-par-recherche-avanc\xe9",level:3},{value:"Choisir la Bonne Approche",id:"choisir-la-bonne-approche",level:2},{value:"Exemple : R\xe9sumer puis Continuer",id:"exemple--r\xe9sumer-puis-continuer",level:2},{value:"Conseils Pratiques",id:"conseils-pratiques",level:2}];function c(e){const s={blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(s.header,{children:(0,i.jsx)(s.h1,{id:"gestion-des-limites-de-tokens-dans-les-conversations-ia",children:"Gestion des Limites de Tokens dans les Conversations IA"})}),"\n",(0,i.jsxs)(s.p,{children:["Lors de l\u2019utilisation d\u2019API de mod\xe8les de langage (LLM) comme OpenAI GPT-4, chaque appel API est soumis \xe0 une ",(0,i.jsx)(s.strong,{children:"limite maximale de tokens"})," (par exemple, 8192 ou 128k tokens). Les conversations longues d\xe9passent facilement cette limite, surtout si l\u2019on conserve l\u2019historique pour assurer la continuit\xe9."]}),"\n",(0,i.jsxs)(s.p,{children:["Ce guide explique ",(0,i.jsx)(s.strong,{children:"pourquoi les limites de tokens sont importantes"})," et propose des strat\xe9gies concr\xe8tes pour pr\xe9server le contexte tout en restant dans les limites impos\xe9es."]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"pourquoi-les-limites-de-tokens-sont-importantes",children:"Pourquoi les Limites de Tokens Sont Importantes"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.strong,{children:"Chaque message de l\u2019historique"})," consomme des tokens (messages utilisateur et assistant)."]}),"\n",(0,i.jsx)(s.li,{children:"Si le total d\xe9passe la limite du mod\xe8le, l\u2019appel API \xe9choue."}),"\n",(0,i.jsxs)(s.li,{children:["Le param\xe8tre ",(0,i.jsx)(s.code,{children:"max_tokens"})," d\xe9finit l\u2019espace restant pour la r\xe9ponse du mod\xe8le."]}),"\n"]}),"\n",(0,i.jsxs)(s.blockquote,{children:["\n",(0,i.jsx)(s.p,{children:"Exemple : Avec GPT-4 (8192 tokens max), si votre prompt fait d\xe9j\xe0 7800 tokens, la r\xe9ponse sera tr\xe8s courte ou l\u2019appel \xe9chouera."}),"\n"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"strat\xe9gies-pour-g\xe9rer-les-limites-de-tokens",children:"Strat\xe9gies pour G\xe9rer les Limites de Tokens"}),"\n",(0,i.jsx)(s.h3,{id:"1-tronquer-les-anciens-messages",children:"1. Tronquer les Anciens Messages"}),"\n",(0,i.jsx)(s.p,{children:"Supprimez les premiers messages de l\u2019historique (sauf le message syst\xe8me)."}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-python",children:"while token_count(messages + [new_prompt]) > MAX_TOKENS - reserved_for_response:\r\n    # Supprime le plus ancien message utilisateur/assistant\r\n    messages.pop(1)\n"})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Avantages :"})," Simple",(0,i.jsx)(s.br,{}),"\n",(0,i.jsx)(s.strong,{children:"Inconv\xe9nients :"})," Perte de contexte initial"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"2-r\xe9sumer-les-anciens-messages",children:"2. R\xe9sumer les Anciens Messages"}),"\n",(0,i.jsx)(s.p,{children:"Avant de supprimer des messages, demandez au mod\xe8le de les r\xe9sumer. Remplacez ensuite le bloc supprim\xe9 par un r\xe9sum\xe9."}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-json",children:'[\r\n  { "role": "system", "content": "Vous \xeates David Goggins..." },\r\n  { "role": "assistant", "content": "R\xe9sum\xe9 : l\'utilisateur a parl\xe9 de faiblesse, de douleur, et vous lui avez conseill\xe9 d\'utiliser la douleur comme moteur." },\r\n  { "role": "user", "content": "Je lutte \xe0 nouveau. Que dois-je faire ?" }\r\n]\n'})}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Avantages :"})," Pr\xe9serve l\u2019intention et la m\xe9moire",(0,i.jsx)(s.br,{}),"\n",(0,i.jsx)(s.strong,{children:"Inconv\xe9nients :"})," N\xe9cessite une logique de r\xe9sum\xe9 ou un appel suppl\xe9mentaire"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"3-strat\xe9gie-hybride-recommand\xe9e",children:"3. Strat\xe9gie Hybride (Recommand\xe9e)"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Conservez les 5 \xe0 10 derniers messages en int\xe9gralit\xe9"}),"\n",(0,i.jsx)(s.li,{children:"R\xe9sumez les parties plus anciennes en un seul message"}),"\n",(0,i.jsx)(s.li,{children:"Combinez contexte brut et r\xe9sum\xe9"}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Id\xe9al pour :"})," Chats longs, jeux de r\xf4le, bots de coaching"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h3,{id:"4-m\xe9moire-augment\xe9e-par-recherche-avanc\xe9",children:"4. M\xe9moire Augment\xe9e par Recherche (Avanc\xe9)"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"Stockez toutes les conversations pass\xe9es en externe (ex. : base de donn\xe9es vectorielle)"}),"\n",(0,i.jsx)(s.li,{children:"R\xe9cup\xe9rez les passages les plus pertinents \xe0 chaque nouvelle requ\xeate"}),"\n",(0,i.jsx)(s.li,{children:"Injectez uniquement ces extraits dans le prompt"}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.strong,{children:"Outils :"})," FAISS, Chroma, LangChain, LlamaIndex",(0,i.jsx)(s.br,{}),"\n",(0,i.jsx)(s.strong,{children:"Avantages :"})," Meilleure \xe9volutivit\xe9 \xe0 long terme",(0,i.jsx)(s.br,{}),"\n",(0,i.jsx)(s.strong,{children:"Inconv\xe9nients :"})," N\xe9cessite une infrastructure backend et une logique d\u2019indexation"]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"choisir-la-bonne-approche",children:"Choisir la Bonne Approche"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"Strat\xe9gie"}),(0,i.jsx)(s.th,{children:"R\xe9tention du contexte"}),(0,i.jsx)(s.th,{children:"Simplicit\xe9"}),(0,i.jsx)(s.th,{children:"Scalabilit\xe9"}),(0,i.jsx)(s.th,{children:"Cas d\u2019usage"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"Suppression des anciens messages"}),(0,i.jsx)(s.td,{children:"\u274c Faible"}),(0,i.jsx)(s.td,{children:"\u2705 Simple"}),(0,i.jsx)(s.td,{children:"\u274c Limit\xe9e"}),(0,i.jsx)(s.td,{children:"T\xe2ches courtes, requ\xeates transactionnelles"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"R\xe9sumer & remplacer"}),(0,i.jsx)(s.td,{children:"\u2705 \xc9lev\xe9e"}),(0,i.jsx)(s.td,{children:"\u26a0\ufe0f Moyenne"}),(0,i.jsx)(s.td,{children:"\u2705 Bonne"}),(0,i.jsx)(s.td,{children:"Coaching, journalisation, chats \xe0 personnalit\xe9"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"Hybride (brut + r\xe9sum\xe9)"}),(0,i.jsx)(s.td,{children:"\u2705\u2705 Excellente"}),(0,i.jsx)(s.td,{children:"\u2696\ufe0f Mod\xe9r\xe9e"}),(0,i.jsx)(s.td,{children:"\u2705\u2705 Meilleure"}),(0,i.jsx)(s.td,{children:"Assistants long terme, bots \xe0 m\xe9moire enrichie"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"M\xe9moire augment\xe9e par recherche"}),(0,i.jsx)(s.td,{children:"\u2705\u2705 Excellente"}),(0,i.jsx)(s.td,{children:"\u274c Complexe"}),(0,i.jsx)(s.td,{children:"\u2705\u2705 Meilleure"}),(0,i.jsx)(s.td,{children:"Applications \xe9volutives avec m\xe9moire ou recherche"})]})]})]}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"exemple--r\xe9sumer-puis-continuer",children:"Exemple : R\xe9sumer puis Continuer"}),"\n",(0,i.jsx)(s.p,{children:"Lorsque l\u2019historique devient trop long :"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsx)(s.li,{children:"R\xe9sumez les anciens messages"}),"\n",(0,i.jsx)(s.li,{children:"Ins\xe9rez le r\xe9sum\xe9 comme message assistant"}),"\n",(0,i.jsx)(s.li,{children:"Continuez la conversation avec les nouveaux prompts"}),"\n"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-python",children:'messages = [\r\n  { "role": "system", "content": "Vous \xeates David Goggins..." },\r\n  { "role": "assistant", "content": "R\xe9sum\xe9 des 50 messages pr\xe9c\xe9dents..." },\r\n  { "role": "user", "content": "Je rechute. Aidez-moi \xe0 repartir." }\r\n]\n'})}),"\n",(0,i.jsx)(s.hr,{}),"\n",(0,i.jsx)(s.h2,{id:"conseils-pratiques",children:"Conseils Pratiques"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["R\xe9servez toujours des ",(0,i.jsx)(s.code,{children:"max_tokens"})," pour la r\xe9ponse de l\u2019assistant (ex. : 300\u20131000 tokens)"]}),"\n",(0,i.jsxs)(s.li,{children:["Utilisez des biblioth\xe8ques comme ",(0,i.jsx)(s.code,{children:"tiktoken"})," pour compter les tokens avant l\u2019envoi"]}),"\n",(0,i.jsx)(s.li,{children:"Actualisez les r\xe9sum\xe9s tous les 20 \xe0 30 messages"}),"\n"]}),"\n",(0,i.jsx)(s.hr,{})]})}function u(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>l,x:()=>o});var t=n(6540);const i={},r=t.createContext(i);function l(e){const s=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function o(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(r.Provider,{value:s},e.children)}}}]);